{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c51e3a",
   "metadata": {},
   "source": [
    "# Experiments: data_analysis\n",
    "\n",
    "**Original File:** `src/document_analyzer/data_analysis.py`\n",
    "\n",
    "## Purpose\n",
    "This module provides document analysis capabilities using LLM-based extraction. It analyzes document text and extracts structured metadata including topics, entities, key points, and summaries.\n",
    "\n",
    "## Key Components\n",
    "- **DocumentAnalyzer class**: Main analyzer for extracting structured metadata\n",
    "  - `analyze_document()`: Analyze document text and return structured metadata\n",
    "  - Uses `JsonOutputParser` with Pydantic model for structured output\n",
    "  - `OutputFixingParser` for automatic error correction\n",
    "\n",
    "## Prerequisites\n",
    "- `langchain`, `langchain-core` installed\n",
    "- Environment variables configured (API keys for LLM)\n",
    "- Pydantic models defined in `model/models.py`\n",
    "- Prompt templates in `prompt/prompt_library.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d832f8d",
   "metadata": {},
   "source": [
    "## Instructions & Setup Guide\n",
    "\n",
    "### Execution Order\n",
    "1. Run the imports cell\n",
    "2. Review the DocumentAnalyzer class definition\n",
    "3. Initialize the analyzer\n",
    "4. Load a document (PDF, text, etc.)\n",
    "5. Call `analyze_document()` with the document text\n",
    "\n",
    "### Dependencies\n",
    "```bash\n",
    "pip install langchain langchain-core python-dotenv pydantic\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "- Ensure `.env` file contains `GROQ_API_KEY` and/or `GOOGLE_API_KEY`\n",
    "- The `Metadata` Pydantic model defines the output structure\n",
    "- Run from project root directory for proper imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efdf844",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies\n",
    "\n",
    "Import all required modules for document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a21f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from model.models import Metadata  # Pydantic model for structured output\n",
    "from prompt.prompt_library import PROMPT_REGISTRY\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b18e1",
   "metadata": {},
   "source": [
    "## 2. DocumentAnalyzer Class Definition\n",
    "\n",
    "The main class for analyzing documents and extracting structured metadata. Key features:\n",
    "- **JsonOutputParser**: Parses LLM output into structured Pydantic model\n",
    "- **OutputFixingParser**: Automatically fixes malformed JSON responses\n",
    "- **Chain pattern**: Uses LCEL for composable processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes documents using a pre-trained model.\n",
    "    Automatically logs all actions and supports session-based organization.\n",
    "    \n",
    "    The analyzer extracts structured metadata from document text including:\n",
    "    - Document title and topics\n",
    "    - Key entities and concepts\n",
    "    - Summary and key points\n",
    "    - Document type classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            # Initialize model loader and LLM\n",
    "            self.loader = ModelLoader()\n",
    "            self.llm = self.loader.load_llm()\n",
    "            \n",
    "            # Prepare parsers for structured output\n",
    "            # JsonOutputParser converts LLM text to Pydantic model\n",
    "            self.parser = JsonOutputParser(pydantic_object=Metadata)\n",
    "            \n",
    "            # OutputFixingParser wraps the parser and can fix malformed JSON\n",
    "            self.fixing_parser = OutputFixingParser.from_llm(\n",
    "                parser=self.parser, \n",
    "                llm=self.llm\n",
    "            )\n",
    "            \n",
    "            # Load the analysis prompt template\n",
    "            self.prompt = PROMPT_REGISTRY[\"document_analysis\"]\n",
    "            \n",
    "            log.info(\"DocumentAnalyzer initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log.error(f\"Error initializing DocumentAnalyzer: {e}\")\n",
    "            raise DocumentPortalException(\"Error in DocumentAnalyzer initialization\", sys)\n",
    "\n",
    "print(\"DocumentAnalyzer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f48fd",
   "metadata": {},
   "source": [
    "## 3. Document Analysis Method\n",
    "\n",
    "The `analyze_document()` method processes document text and returns structured metadata.\n",
    "\n",
    "### How it works:\n",
    "1. Creates an LCEL chain: `prompt | llm | fixing_parser`\n",
    "2. Passes document text and format instructions to the chain\n",
    "3. LLM generates structured JSON output\n",
    "4. Parser validates and converts to Python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20693a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(self, document_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze a document's text and extract structured metadata & summary.\n",
    "    \n",
    "    Args:\n",
    "        document_text: The full text content of the document to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Structured metadata including title, topics, entities,\n",
    "              summary, key points, and document type\n",
    "    \n",
    "    Raises:\n",
    "        DocumentPortalException: If metadata extraction fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build the LCEL chain: prompt -> LLM -> parser\n",
    "        chain = self.prompt | self.llm | self.fixing_parser\n",
    "        \n",
    "        log.info(\"Meta-data analysis chain initialized\")\n",
    "\n",
    "        # Invoke the chain with document text and format instructions\n",
    "        response = chain.invoke({\n",
    "            \"format_instructions\": self.parser.get_format_instructions(),\n",
    "            \"document_text\": document_text\n",
    "        })\n",
    "\n",
    "        log.info(\"Metadata extraction successful\", keys=list(response.keys()))\n",
    "        \n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(\"Metadata analysis failed\", error=str(e))\n",
    "        raise DocumentPortalException(\"Metadata extraction failed\", sys)\n",
    "\n",
    "# Attach method to class\n",
    "DocumentAnalyzer.analyze_document = analyze_document\n",
    "print(\"analyze_document method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c080b14",
   "metadata": {},
   "source": [
    "## 4. Understanding the Metadata Model\n",
    "\n",
    "Let's examine what the `Metadata` Pydantic model looks like. This defines the structure of the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Metadata model schema\n",
    "from model.models import Metadata\n",
    "\n",
    "print(\"Metadata Model Schema:\")\n",
    "print(\"-\" * 50)\n",
    "print(Metadata.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2041a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the format instructions that are sent to the LLM\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Metadata)\n",
    "print(\"Format Instructions for LLM:\")\n",
    "print(\"-\" * 50)\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f51ef",
   "metadata": {},
   "source": [
    "## 5. Usage Example\n",
    "\n",
    "Demonstrate how to use the DocumentAnalyzer with sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c13f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = DocumentAnalyzer()\n",
    "print(\"Analyzer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15935c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document text for analysis\n",
    "sample_document = \"\"\"\n",
    "Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence (AI) that enables systems to learn \n",
    "and improve from experience without being explicitly programmed. It focuses on developing \n",
    "computer programs that can access data and use it to learn for themselves.\n",
    "\n",
    "The process begins with observations or data, such as examples, direct experience, or \n",
    "instruction, to look for patterns in data and make better decisions in the future. The \n",
    "primary aim is to allow computers to learn automatically without human intervention.\n",
    "\n",
    "Key Concepts:\n",
    "1. Supervised Learning - Learning from labeled training data\n",
    "2. Unsupervised Learning - Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning - Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, natural language processing, recommendation systems,\n",
    "and autonomous vehicles. Major companies like Google, Amazon, and Microsoft heavily invest\n",
    "in machine learning research and applications.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(sample_document)} characters\")\n",
    "print(\"\\nFirst 200 characters:\")\n",
    "print(sample_document[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7214e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the document\n",
    "result = analyzer.analyze_document(sample_document)\n",
    "\n",
    "print(\"Analysis Result:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in result.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    if isinstance(value, list):\n",
    "        for item in value:\n",
    "            print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5441d4",
   "metadata": {},
   "source": [
    "## 6. Working with PDF Documents\n",
    "\n",
    "Example of analyzing a PDF document using the DocHandler from data_ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document_ingestion.data_ingestion import DocHandler\n",
    "\n",
    "# Initialize document handler\n",
    "doc_handler = DocHandler(session_id=\"analysis_demo\")\n",
    "print(f\"DocHandler initialized with session: {doc_handler.session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze a PDF file (update path as needed)\n",
    "PDF_PATH = \"data/document_analysis/sample.pdf\"  # Update this path\n",
    "\n",
    "if os.path.exists(PDF_PATH):\n",
    "    # Read PDF content\n",
    "    pdf_text = doc_handler.read_pdf(PDF_PATH)\n",
    "    print(f\"PDF loaded: {len(pdf_text)} characters\")\n",
    "    \n",
    "    # Analyze the PDF\n",
    "    pdf_result = analyzer.analyze_document(pdf_text)\n",
    "    print(\"\\nPDF Analysis Result:\")\n",
    "    print(pdf_result)\n",
    "else:\n",
    "    print(f\"PDF not found at: {PDF_PATH}\")\n",
    "    print(\"Skipping PDF analysis example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34386574",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. **DocumentAnalyzer** extracts structured metadata from document text using LLM\n",
    "2. Uses **JsonOutputParser** with Pydantic models for type-safe structured output\n",
    "3. **OutputFixingParser** automatically corrects malformed JSON responses\n",
    "4. The LCEL chain pattern (`prompt | llm | parser`) enables composable processing\n",
    "\n",
    "### Possible Extensions\n",
    "- Add support for different analysis types (legal, medical, technical)\n",
    "- Implement batch processing for multiple documents\n",
    "- Add confidence scores to extracted metadata\n",
    "- Support for multilingual document analysis\n",
    "- Integration with document classification systems"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
