{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f36508",
   "metadata": {},
   "source": [
    "# Experiments: data_ingestion\n",
    "\n",
    "**Original File:** `src/document_ingestion/data_ingestion.py`\n",
    "\n",
    "## Purpose\n",
    "This module provides comprehensive document ingestion capabilities for the Document Portal. It handles file upload, text extraction, chunking, FAISS index management, and session-based document organization.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. FaissManager\n",
    "- Manages FAISS vector index creation and loading\n",
    "- Supports incremental document addition with deduplication\n",
    "- Persists metadata for tracking ingested documents\n",
    "\n",
    "### 2. ChatIngestor\n",
    "- Main ingestion pipeline for RAG/chat applications\n",
    "- Handles file upload, document loading, and chunking\n",
    "- Builds retrievers for question answering\n",
    "\n",
    "### 3. DocHandler\n",
    "- PDF save and read operations for document analysis\n",
    "- Session-based file organization\n",
    "\n",
    "### 4. DocumentComparator\n",
    "- Save, read, and combine PDFs for comparison\n",
    "- Session-based versioning and cleanup\n",
    "\n",
    "## Prerequisites\n",
    "- `langchain`, `langchain-community`, `faiss-cpu` installed\n",
    "- `PyMuPDF` (fitz) for PDF processing\n",
    "- Environment variables configured (API keys)\n",
    "- Utility modules: `file_io`, `document_ops`, `model_loader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133aa13",
   "metadata": {},
   "source": [
    "## Instructions & Setup Guide\n",
    "\n",
    "### Execution Order\n",
    "1. Run the imports cell\n",
    "2. Explore FaissManager for vector index operations\n",
    "3. Use ChatIngestor to build a RAG retriever\n",
    "4. Use DocHandler for document analysis workflows\n",
    "5. Use DocumentComparator for document comparison workflows\n",
    "\n",
    "### Dependencies\n",
    "```bash\n",
    "pip install langchain langchain-community faiss-cpu PyMuPDF python-dotenv\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "- Ensure `.env` file contains `GROQ_API_KEY` and/or `GOOGLE_API_KEY`\n",
    "- Default directories: `data/`, `faiss_index/`\n",
    "- Run from project root directory for proper imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405b41b",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies\n",
    "\n",
    "Import all required modules for document ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Dict, Any\n",
    "\n",
    "# PDF processing\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from utils.file_io import generate_session_id, save_uploaded_files\n",
    "from utils.document_ops import load_documents, concat_for_analysis, concat_for_comparison\n",
    "\n",
    "# Supported file types\n",
    "SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Supported file extensions: {SUPPORTED_EXTENSIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e90ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: FaissManager Class\n",
    "\n",
    "The `FaissManager` class handles FAISS vector index operations:\n",
    "- Create new indexes from text\n",
    "- Load existing indexes\n",
    "- Add documents incrementally with deduplication\n",
    "- Persist metadata for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissManager:\n",
    "    \"\"\"\n",
    "    FAISS vector index manager with load-or-create pattern.\n",
    "    \n",
    "    Features:\n",
    "    - Automatic index creation/loading\n",
    "    - Incremental document addition\n",
    "    - Deduplication via fingerprinting\n",
    "    - Metadata persistence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_dir: Path, model_loader: Optional[ModelLoader] = None):\n",
    "        \"\"\"\n",
    "        Initialize FaissManager.\n",
    "        \n",
    "        Args:\n",
    "            index_dir: Directory to store FAISS index files\n",
    "            model_loader: Optional ModelLoader instance (creates new if not provided)\n",
    "        \"\"\"\n",
    "        self.index_dir = Path(index_dir)\n",
    "        self.index_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Metadata file tracks ingested documents\n",
    "        self.meta_path = self.index_dir / \"ingested_meta.json\"\n",
    "        self._meta: Dict[str, Any] = {\"rows\": {}}  # Dict of fingerprint -> True\n",
    "        \n",
    "        # Load existing metadata if available\n",
    "        if self.meta_path.exists():\n",
    "            try:\n",
    "                self._meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\")) or {\"rows\": {}}\n",
    "            except Exception:\n",
    "                self._meta = {\"rows\": {}}\n",
    "        \n",
    "        # Initialize embeddings model\n",
    "        self.model_loader = model_loader or ModelLoader()\n",
    "        self.emb = self.model_loader.load_embeddings()\n",
    "        self.vs: Optional[FAISS] = None\n",
    "        \n",
    "    def _exists(self) -> bool:\n",
    "        \"\"\"Check if FAISS index files exist.\"\"\"\n",
    "        return (self.index_dir / \"index.faiss\").exists() and (self.index_dir / \"index.pkl\").exists()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fingerprint(text: str, md: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a unique fingerprint for a document chunk.\n",
    "        Uses source file path + row_id if available, else content hash.\n",
    "        \"\"\"\n",
    "        src = md.get(\"source\") or md.get(\"file_path\")\n",
    "        rid = md.get(\"row_id\")\n",
    "        if src is not None:\n",
    "            return f\"{src}::{'' if rid is None else rid}\"\n",
    "        return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "    \n",
    "    def _save_meta(self):\n",
    "        \"\"\"Persist metadata to disk.\"\"\"\n",
    "        self.meta_path.write_text(\n",
    "            json.dumps(self._meta, ensure_ascii=False, indent=2), \n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "print(\"FaissManager class defined (Part 1)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7da31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add methods to FaissManager\n",
    "\n",
    "def add_documents(self, docs: List[Document]):\n",
    "    \"\"\"\n",
    "    Add documents to the index with deduplication.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of LangChain Document objects\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of new documents added\n",
    "    \"\"\"\n",
    "    if self.vs is None:\n",
    "        raise RuntimeError(\"Call load_or_create() before add_documents().\")\n",
    "    \n",
    "    new_docs: List[Document] = []\n",
    "    \n",
    "    for d in docs:\n",
    "        # Generate fingerprint for deduplication\n",
    "        key = self._fingerprint(d.page_content, d.metadata or {})\n",
    "        if key in self._meta[\"rows\"]:\n",
    "            continue  # Skip already ingested document\n",
    "        self._meta[\"rows\"][key] = True\n",
    "        new_docs.append(d)\n",
    "        \n",
    "    if new_docs:\n",
    "        self.vs.add_documents(new_docs)\n",
    "        self.vs.save_local(str(self.index_dir))\n",
    "        self._save_meta()\n",
    "        \n",
    "    return len(new_docs)\n",
    "\n",
    "FaissManager.add_documents = add_documents\n",
    "\n",
    "\n",
    "def load_or_create(self, texts: Optional[List[str]] = None, \n",
    "                   metadatas: Optional[List[dict]] = None):\n",
    "    \"\"\"\n",
    "    Load existing FAISS index or create new one from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings (required for new index)\n",
    "        metadatas: Optional metadata for each text\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: The loaded or created vectorstore\n",
    "    \"\"\"\n",
    "    # Try to load existing index\n",
    "    if self._exists():\n",
    "        self.vs = FAISS.load_local(\n",
    "            str(self.index_dir),\n",
    "            embeddings=self.emb,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "        return self.vs\n",
    "    \n",
    "    # Create new index from texts\n",
    "    if not texts:\n",
    "        raise DocumentPortalException(\n",
    "            \"No existing FAISS index and no data to create one\", sys\n",
    "        )\n",
    "    \n",
    "    self.vs = FAISS.from_texts(\n",
    "        texts=texts, \n",
    "        embedding=self.emb, \n",
    "        metadatas=metadatas or []\n",
    "    )\n",
    "    self.vs.save_local(str(self.index_dir))\n",
    "    return self.vs\n",
    "\n",
    "FaissManager.load_or_create = load_or_create\n",
    "\n",
    "print(\"FaissManager methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e52dc",
   "metadata": {},
   "source": [
    "### FaissManager Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252695f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a new FAISS index\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize FaissManager\n",
    "index_dir = Path(\"faiss_index/demo_index\")\n",
    "fm = FaissManager(index_dir)\n",
    "print(f\"FaissManager initialized at: {index_dir}\")\n",
    "print(f\"Index exists: {fm._exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b500c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index with sample texts\n",
    "sample_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with many layers.\",\n",
    "    \"Natural language processing deals with text and speech.\",\n",
    "    \"Computer vision enables machines to interpret images.\",\n",
    "]\n",
    "\n",
    "sample_metas = [\n",
    "    {\"source\": \"intro.txt\", \"topic\": \"ML\"},\n",
    "    {\"source\": \"deep_learning.txt\", \"topic\": \"DL\"},\n",
    "    {\"source\": \"nlp.txt\", \"topic\": \"NLP\"},\n",
    "    {\"source\": \"cv.txt\", \"topic\": \"CV\"},\n",
    "]\n",
    "\n",
    "vs = fm.load_or_create(texts=sample_texts, metadatas=sample_metas)\n",
    "print(f\"Vectorstore created with {len(sample_texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search\n",
    "query = \"What is deep learning?\"\n",
    "results = vs.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Result {i+1}: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe85fc",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: ChatIngestor Class\n",
    "\n",
    "The `ChatIngestor` class provides the main ingestion pipeline for RAG applications:\n",
    "- File upload handling\n",
    "- Document loading and chunking\n",
    "- Retriever building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatIngestor:\n",
    "    \"\"\"\n",
    "    Main ingestion pipeline for RAG/chat applications.\n",
    "    \n",
    "    Features:\n",
    "    - Handles file uploads (PDF, DOCX, TXT)\n",
    "    - Document chunking with configurable size/overlap\n",
    "    - Session-based directory organization\n",
    "    - Builds FAISS retrievers for question answering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        temp_base: str = \"data\",\n",
    "        faiss_base: str = \"faiss_index\",\n",
    "        use_session_dirs: bool = True,\n",
    "        session_id: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ChatIngestor.\n",
    "        \n",
    "        Args:\n",
    "            temp_base: Base directory for temporary file storage\n",
    "            faiss_base: Base directory for FAISS indexes\n",
    "            use_session_dirs: Whether to create session subdirectories\n",
    "            session_id: Optional session ID (auto-generated if not provided)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model_loader = ModelLoader()\n",
    "            \n",
    "            self.use_session = use_session_dirs\n",
    "            self.session_id = session_id or generate_session_id()\n",
    "            \n",
    "            # Set up directories\n",
    "            self.temp_base = Path(temp_base)\n",
    "            self.temp_base.mkdir(parents=True, exist_ok=True)\n",
    "            self.faiss_base = Path(faiss_base)\n",
    "            self.faiss_base.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            self.temp_dir = self._resolve_dir(self.temp_base)\n",
    "            self.faiss_dir = self._resolve_dir(self.faiss_base)\n",
    "\n",
    "            log.info(\"ChatIngestor initialized\",\n",
    "                     session_id=self.session_id,\n",
    "                     temp_dir=str(self.temp_dir),\n",
    "                     faiss_dir=str(self.faiss_dir),\n",
    "                     sessionized=self.use_session)\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to initialize ChatIngestor\", error=str(e))\n",
    "            raise DocumentPortalException(\"Initialization error in ChatIngestor\", e) from e\n",
    "            \n",
    "    def _resolve_dir(self, base: Path):\n",
    "        \"\"\"Resolve directory path with optional session subdirectory.\"\"\"\n",
    "        if self.use_session:\n",
    "            d = base / self.session_id\n",
    "            d.mkdir(parents=True, exist_ok=True)\n",
    "            return d\n",
    "        return base\n",
    "        \n",
    "    def _split(self, docs: List[Document], chunk_size=1000, chunk_overlap=200) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks.\"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        log.info(\"Documents split\", chunks=len(chunks), chunk_size=chunk_size, overlap=chunk_overlap)\n",
    "        return chunks\n",
    "\n",
    "print(\"ChatIngestor class defined (Part 1)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add build_retriever method to ChatIngestor\n",
    "\n",
    "def built_retriver(\n",
    "    self,\n",
    "    uploaded_files: Iterable,\n",
    "    *,\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    k: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a retriever from uploaded files.\n",
    "    \n",
    "    Args:\n",
    "        uploaded_files: Iterable of file objects (e.g., from Streamlit/FastAPI)\n",
    "        chunk_size: Size of text chunks (default: 1000)\n",
    "        chunk_overlap: Overlap between chunks (default: 200)\n",
    "        k: Number of documents to retrieve (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        Retriever: Configured FAISS retriever\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save uploaded files to temp directory\n",
    "        paths = save_uploaded_files(uploaded_files, self.temp_dir)\n",
    "        \n",
    "        # Load documents from saved files\n",
    "        docs = load_documents(paths)\n",
    "        if not docs:\n",
    "            raise ValueError(\"No valid documents loaded\")\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        chunks = self._split(docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        \n",
    "        # Initialize FAISS manager\n",
    "        fm = FaissManager(self.faiss_dir, self.model_loader)\n",
    "        \n",
    "        # Extract texts and metadata from chunks\n",
    "        texts = [c.page_content for c in chunks]\n",
    "        metas = [c.metadata for c in chunks]\n",
    "        \n",
    "        # Create or load vectorstore\n",
    "        vs = fm.load_or_create(texts=texts, metadatas=metas)\n",
    "        \n",
    "        # Add documents (handles deduplication)\n",
    "        added = fm.add_documents(chunks)\n",
    "        log.info(\"FAISS index updated\", added=added, index=str(self.faiss_dir))\n",
    "        \n",
    "        # Return retriever\n",
    "        return vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "        \n",
    "    except Exception as e:\n",
    "        log.error(\"Failed to build retriever\", error=str(e))\n",
    "        raise DocumentPortalException(\"Failed to build retriever\", e) from e\n",
    "\n",
    "ChatIngestor.built_retriver = built_retriver\n",
    "print(\"build_retriever method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d4b8d",
   "metadata": {},
   "source": [
    "### ChatIngestor Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatIngestor\n",
    "ingestor = ChatIngestor(\n",
    "    temp_base=\"data/multi_doc_chat\",\n",
    "    faiss_base=\"faiss_index\",\n",
    "    session_id=\"notebook_demo\"\n",
    ")\n",
    "\n",
    "print(f\"Session ID: {ingestor.session_id}\")\n",
    "print(f\"Temp directory: {ingestor.temp_dir}\")\n",
    "print(f\"FAISS directory: {ingestor.faiss_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de565e53",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: DocHandler Class\n",
    "\n",
    "The `DocHandler` class provides PDF operations for document analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323972df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocHandler:\n",
    "    \"\"\"\n",
    "    PDF save + read (page-wise) for analysis workflows.\n",
    "    \n",
    "    Features:\n",
    "    - Save uploaded PDFs to session directory\n",
    "    - Read PDF content with page separation\n",
    "    - Session-based file organization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Optional[str] = None, session_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize DocHandler.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Base directory for document storage\n",
    "            session_id: Optional session ID (auto-generated if not provided)\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir or os.getenv(\n",
    "            \"DATA_STORAGE_PATH\", \n",
    "            os.path.join(os.getcwd(), \"data\", \"document_analysis\")\n",
    "        )\n",
    "        self.session_id = session_id or generate_session_id(\"session\")\n",
    "        self.session_path = os.path.join(self.data_dir, self.session_id)\n",
    "        os.makedirs(self.session_path, exist_ok=True)\n",
    "        log.info(\"DocHandler initialized\", \n",
    "                 session_id=self.session_id, \n",
    "                 session_path=self.session_path)\n",
    "\n",
    "    def save_pdf(self, uploaded_file) -> str:\n",
    "        \"\"\"\n",
    "        Save uploaded PDF to session directory.\n",
    "        \n",
    "        Args:\n",
    "            uploaded_file: File object with name and content\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to saved file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.basename(uploaded_file.name)\n",
    "            if not filename.lower().endswith(\".pdf\"):\n",
    "                raise ValueError(\"Invalid file type. Only PDFs are allowed.\")\n",
    "            save_path = os.path.join(self.session_path, filename)\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                if hasattr(uploaded_file, \"read\"):\n",
    "                    f.write(uploaded_file.read())\n",
    "                else:\n",
    "                    f.write(uploaded_file.getbuffer())\n",
    "            log.info(\"PDF saved successfully\", \n",
    "                     file=filename, \n",
    "                     save_path=save_path, \n",
    "                     session_id=self.session_id)\n",
    "            return save_path\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to save PDF\", error=str(e), session_id=self.session_id)\n",
    "            raise DocumentPortalException(f\"Failed to save PDF: {str(e)}\", e) from e\n",
    "\n",
    "    def read_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Read PDF content with page separation.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "        \n",
    "        Returns:\n",
    "            str: Full text content with page markers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text_chunks = []\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page_num in range(doc.page_count):\n",
    "                    page = doc.load_page(page_num)\n",
    "                    text_chunks.append(\n",
    "                        f\"\\n--- Page {page_num + 1} ---\\n{page.get_text()}\"\n",
    "                    )\n",
    "            text = \"\\n\".join(text_chunks)\n",
    "            log.info(\"PDF read successfully\", \n",
    "                     pdf_path=pdf_path, \n",
    "                     session_id=self.session_id, \n",
    "                     pages=len(text_chunks))\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to read PDF\", \n",
    "                      error=str(e), \n",
    "                      pdf_path=pdf_path, \n",
    "                      session_id=self.session_id)\n",
    "            raise DocumentPortalException(f\"Could not process PDF: {pdf_path}\", e) from e\n",
    "\n",
    "print(\"DocHandler class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36246c47",
   "metadata": {},
   "source": [
    "### DocHandler Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DocHandler\n",
    "doc_handler = DocHandler(session_id=\"analysis_notebook_demo\")\n",
    "\n",
    "print(f\"Session ID: {doc_handler.session_id}\")\n",
    "print(f\"Session path: {doc_handler.session_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86459e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read an existing PDF\n",
    "import os\n",
    "\n",
    "# Look for any PDF in the data directory\n",
    "data_dir = \"data/multi_doc_chat\"\n",
    "if os.path.exists(data_dir):\n",
    "    pdfs = [f for f in os.listdir(data_dir) if f.endswith('.txt') or f.endswith('.pdf')]\n",
    "    if pdfs:\n",
    "        sample_path = os.path.join(data_dir, pdfs[0])\n",
    "        print(f\"Found file: {sample_path}\")\n",
    "        \n",
    "        if sample_path.endswith('.txt'):\n",
    "            with open(sample_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            print(f\"\\nFirst 500 characters:\\n{content[:500]}...\")\n",
    "    else:\n",
    "        print(\"No sample files found.\")\n",
    "else:\n",
    "    print(f\"Directory not found: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4020c57",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: DocumentComparator Class\n",
    "\n",
    "The `DocumentComparator` class handles file operations for document comparison workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3663621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentComparator:\n",
    "    \"\"\"\n",
    "    Save, read & combine PDFs for comparison with session-based versioning.\n",
    "    \n",
    "    Features:\n",
    "    - Save reference and actual documents to session\n",
    "    - Read individual PDFs with page markers\n",
    "    - Combine all PDFs in session for comparison\n",
    "    - Clean up old sessions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"data/document_compare\", \n",
    "                 session_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize DocumentComparator.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory for comparison sessions\n",
    "            session_id: Optional session ID (auto-generated if not provided)\n",
    "        \"\"\"\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.session_id = session_id or generate_session_id()\n",
    "        self.session_path = self.base_dir / self.session_id\n",
    "        self.session_path.mkdir(parents=True, exist_ok=True)\n",
    "        log.info(\"DocumentComparator initialized\", session_path=str(self.session_path))\n",
    "\n",
    "    def save_uploaded_files(self, reference_file, actual_file):\n",
    "        \"\"\"\n",
    "        Save reference and actual PDF files.\n",
    "        \n",
    "        Args:\n",
    "            reference_file: Reference document file object\n",
    "            actual_file: Actual document file object\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Paths to saved reference and actual files\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ref_path = self.session_path / reference_file.name\n",
    "            act_path = self.session_path / actual_file.name\n",
    "            \n",
    "            for fobj, out in ((reference_file, ref_path), (actual_file, act_path)):\n",
    "                if not fobj.name.lower().endswith(\".pdf\"):\n",
    "                    raise ValueError(\"Only PDF files are allowed.\")\n",
    "                with open(out, \"wb\") as f:\n",
    "                    if hasattr(fobj, \"read\"):\n",
    "                        f.write(fobj.read())\n",
    "                    else:\n",
    "                        f.write(fobj.getbuffer())\n",
    "                        \n",
    "            log.info(\"Files saved\", \n",
    "                     reference=str(ref_path), \n",
    "                     actual=str(act_path), \n",
    "                     session=self.session_id)\n",
    "            return ref_path, act_path\n",
    "        except Exception as e:\n",
    "            log.error(\"Error saving PDF files\", error=str(e), session=self.session_id)\n",
    "            raise DocumentPortalException(\"Error saving files\", e) from e\n",
    "\n",
    "    def read_pdf(self, pdf_path: Path) -> str:\n",
    "        \"\"\"\n",
    "        Read PDF content with page markers.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "        \n",
    "        Returns:\n",
    "            str: Text content with page separators\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                if doc.is_encrypted:\n",
    "                    raise ValueError(f\"PDF is encrypted: {pdf_path.name}\")\n",
    "                parts = []\n",
    "                for page_num in range(doc.page_count):\n",
    "                    page = doc.load_page(page_num)\n",
    "                    text = page.get_text()\n",
    "                    if text.strip():\n",
    "                        parts.append(f\"\\n --- Page {page_num + 1} --- \\n{text}\")\n",
    "            log.info(\"PDF read successfully\", file=str(pdf_path), pages=len(parts))\n",
    "            return \"\\n\".join(parts)\n",
    "        except Exception as e:\n",
    "            log.error(\"Error reading PDF\", file=str(pdf_path), error=str(e))\n",
    "            raise DocumentPortalException(\"Error reading PDF\", e) from e\n",
    "\n",
    "    def combine_documents(self) -> str:\n",
    "        \"\"\"\n",
    "        Combine all PDFs in session directory.\n",
    "        \n",
    "        Returns:\n",
    "            str: Combined text from all documents with labels\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc_parts = []\n",
    "            for file in sorted(self.session_path.iterdir()):\n",
    "                if file.is_file() and file.suffix.lower() == \".pdf\":\n",
    "                    content = self.read_pdf(file)\n",
    "                    doc_parts.append(f\"Document: {file.name}\\n{content}\")\n",
    "            combined_text = \"\\n\\n\".join(doc_parts)\n",
    "            log.info(\"Documents combined\", count=len(doc_parts), session=self.session_id)\n",
    "            return combined_text\n",
    "        except Exception as e:\n",
    "            log.error(\"Error combining documents\", error=str(e), session=self.session_id)\n",
    "            raise DocumentPortalException(\"Error combining documents\", e) from e\n",
    "\n",
    "    def clean_old_sessions(self, keep_latest: int = 3):\n",
    "        \"\"\"\n",
    "        Clean up old session directories.\n",
    "        \n",
    "        Args:\n",
    "            keep_latest: Number of most recent sessions to keep\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sessions = sorted(\n",
    "                [f for f in self.base_dir.iterdir() if f.is_dir()], \n",
    "                reverse=True\n",
    "            )\n",
    "            for folder in sessions[keep_latest:]:\n",
    "                shutil.rmtree(folder, ignore_errors=True)\n",
    "                log.info(\"Old session folder deleted\", path=str(folder))\n",
    "        except Exception as e:\n",
    "            log.error(\"Error cleaning old sessions\", error=str(e))\n",
    "            raise DocumentPortalException(\"Error cleaning old sessions\", e) from e\n",
    "\n",
    "print(\"DocumentComparator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb5622",
   "metadata": {},
   "source": [
    "### DocumentComparator Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb879cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DocumentComparator\n",
    "comparator = DocumentComparator(session_id=\"compare_notebook_demo\")\n",
    "\n",
    "print(f\"Session ID: {comparator.session_id}\")\n",
    "print(f\"Session path: {comparator.session_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ce0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing sessions\n",
    "if comparator.base_dir.exists():\n",
    "    sessions = [d.name for d in comparator.base_dir.iterdir() if d.is_dir()]\n",
    "    print(f\"Existing sessions: {sessions}\")\n",
    "else:\n",
    "    print(\"No comparison sessions exist yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735d9c2",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **FaissManager** provides robust vector index management:\n",
    "   - Load-or-create pattern for index handling\n",
    "   - Incremental document addition with deduplication\n",
    "   - Metadata persistence for tracking\n",
    "\n",
    "2. **ChatIngestor** is the main RAG ingestion pipeline:\n",
    "   - Handles file upload and document loading\n",
    "   - Configurable chunking (size, overlap)\n",
    "   - Session-based organization\n",
    "\n",
    "3. **DocHandler** handles PDF operations for analysis:\n",
    "   - Save uploaded PDFs\n",
    "   - Read with page separation\n",
    "\n",
    "4. **DocumentComparator** manages document comparison workflows:\n",
    "   - Save reference and actual documents\n",
    "   - Combine documents for LLM comparison\n",
    "   - Session cleanup utilities\n",
    "\n",
    "### Possible Extensions\n",
    "- Add support for more file formats (PPTX, XLSX)\n",
    "- Implement parallel document processing\n",
    "- Add OCR support for scanned PDFs\n",
    "- Implement index versioning and rollback\n",
    "- Add document metadata extraction during ingestion\n",
    "- Support for cloud storage backends (S3, GCS)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
