{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8e7a01",
   "metadata": {},
   "source": [
    "# Experiments: document_comparator\n",
    "\n",
    "**Original File:** `src/document_compare/document_comparator.py`\n",
    "\n",
    "## Purpose\n",
    "This module provides LLM-powered document comparison capabilities. It compares multiple documents and generates structured comparison results highlighting similarities, differences, and key insights.\n",
    "\n",
    "## Key Components\n",
    "- **DocumentComparatorLLM class**: Main comparator using LLM for intelligent comparison\n",
    "  - `compare_documents()`: Compare combined document text and return structured results\n",
    "  - `_format_response()`: Convert comparison results to pandas DataFrame\n",
    "  - Uses `JsonOutputParser` for structured output\n",
    "\n",
    "## Prerequisites\n",
    "- `langchain`, `langchain-core`, `pandas` installed\n",
    "- Environment variables configured (API keys for LLM)\n",
    "- `SummaryResponse` Pydantic model in `model/models.py`\n",
    "- Comparison prompt template in `prompt/prompt_library.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e54ed",
   "metadata": {},
   "source": [
    "## Instructions & Setup Guide\n",
    "\n",
    "### Execution Order\n",
    "1. Run the imports cell\n",
    "2. Review the DocumentComparatorLLM class definition\n",
    "3. Initialize the comparator\n",
    "4. Prepare document text (combine documents to compare)\n",
    "5. Call `compare_documents()` with the combined text\n",
    "\n",
    "### Dependencies\n",
    "```bash\n",
    "pip install langchain langchain-core pandas python-dotenv pydantic\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "- Ensure `.env` file contains `GROQ_API_KEY` and/or `GOOGLE_API_KEY`\n",
    "- The `SummaryResponse` Pydantic model defines the comparison output structure\n",
    "- Run from project root directory for proper imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99532c28",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies\n",
    "\n",
    "Import all required modules for document comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from prompt.prompt_library import PROMPT_REGISTRY\n",
    "from model.models import SummaryResponse, PromptType\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ced59",
   "metadata": {},
   "source": [
    "## 2. DocumentComparatorLLM Class Definition\n",
    "\n",
    "The main class for comparing documents using LLM. Key features:\n",
    "- **JsonOutputParser**: Parses LLM output into structured format\n",
    "- **OutputFixingParser**: Available for error correction if needed\n",
    "- **DataFrame output**: Results formatted as pandas DataFrame for easy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentComparatorLLM:\n",
    "    \"\"\"\n",
    "    LLM-based document comparator that analyzes and compares multiple documents.\n",
    "    \n",
    "    The comparator:\n",
    "    - Takes combined document text as input\n",
    "    - Uses LLM to identify similarities and differences\n",
    "    - Returns structured comparison results as a DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Initialize model loader and LLM\n",
    "        self.loader = ModelLoader()\n",
    "        self.llm = self.loader.load_llm()\n",
    "        \n",
    "        # Set up JSON parser with SummaryResponse Pydantic model\n",
    "        self.parser = JsonOutputParser(pydantic_object=SummaryResponse)\n",
    "        \n",
    "        # OutputFixingParser for handling malformed JSON\n",
    "        self.fixing_parser = OutputFixingParser.from_llm(\n",
    "            parser=self.parser, \n",
    "            llm=self.llm\n",
    "        )\n",
    "        \n",
    "        # Load comparison prompt template\n",
    "        self.prompt = PROMPT_REGISTRY[PromptType.DOCUMENT_COMPARISON.value]\n",
    "        \n",
    "        # Build the LCEL chain: prompt -> LLM -> parser\n",
    "        self.chain = self.prompt | self.llm | self.parser\n",
    "        \n",
    "        log.info(\"DocumentComparatorLLM initialized\", model=self.llm)\n",
    "\n",
    "print(\"DocumentComparatorLLM class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff616a8",
   "metadata": {},
   "source": [
    "## 3. Document Comparison Methods\n",
    "\n",
    "The `compare_documents()` method processes combined document text and returns comparison results.\n",
    "\n",
    "### How it works:\n",
    "1. Takes combined document text (multiple docs concatenated with labels)\n",
    "2. Invokes the LCEL chain with the text and format instructions\n",
    "3. LLM analyzes and compares the documents\n",
    "4. Results are formatted into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_documents(self, combined_docs: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare documents and return structured comparison results.\n",
    "    \n",
    "    Args:\n",
    "        combined_docs: Combined text of all documents to compare,\n",
    "                      typically with document labels/separators\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Comparison results with columns for aspects,\n",
    "                     document details, similarities, and differences\n",
    "    \n",
    "    Raises:\n",
    "        DocumentPortalException: If comparison fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare inputs for the chain\n",
    "        inputs = {\n",
    "            \"combined_docs\": combined_docs,\n",
    "            \"format_instruction\": self.parser.get_format_instructions()\n",
    "        }\n",
    "\n",
    "        log.info(\"Invoking document comparison LLM chain\")\n",
    "        \n",
    "        # Invoke the chain\n",
    "        response = self.chain.invoke(inputs)\n",
    "        \n",
    "        log.info(\"Chain invoked successfully\", response_preview=str(response)[:200])\n",
    "        \n",
    "        # Format and return as DataFrame\n",
    "        return self._format_response(response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log.error(\"Error in compare_documents\", error=str(e))\n",
    "        raise DocumentPortalException(\"Error comparing documents\", sys)\n",
    "\n",
    "# Attach to class\n",
    "DocumentComparatorLLM.compare_documents = compare_documents\n",
    "print(\"compare_documents method added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f73a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_response(self, response_parsed: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the parsed response into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        response_parsed: List of dictionaries with comparison results\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Formatted comparison results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(response_parsed)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log.error(\"Error formatting response into DataFrame\", error=str(e))\n",
    "        raise DocumentPortalException(\"Error formatting response\", sys)\n",
    "\n",
    "# Attach to class\n",
    "DocumentComparatorLLM._format_response = _format_response\n",
    "print(\"_format_response method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7437f919",
   "metadata": {},
   "source": [
    "## 4. Understanding the SummaryResponse Model\n",
    "\n",
    "Let's examine what the `SummaryResponse` Pydantic model looks like. This defines the structure of the comparison output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the SummaryResponse model schema\n",
    "from model.models import SummaryResponse\n",
    "\n",
    "print(\"SummaryResponse Model Schema:\")\n",
    "print(\"-\" * 50)\n",
    "print(SummaryResponse.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the format instructions that are sent to the LLM\n",
    "parser = JsonOutputParser(pydantic_object=SummaryResponse)\n",
    "print(\"Format Instructions for LLM:\")\n",
    "print(\"-\" * 50)\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff835f51",
   "metadata": {},
   "source": [
    "## 5. Usage Example\n",
    "\n",
    "Demonstrate how to use the DocumentComparatorLLM with sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddaafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the comparator\n",
    "comparator = DocumentComparatorLLM()\n",
    "print(\"Comparator initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for comparison\n",
    "document_1 = \"\"\"\n",
    "Document: Company_Policy_2024.pdf\n",
    "\n",
    "Employee Vacation Policy\n",
    "\n",
    "1. Annual Leave: Employees are entitled to 20 days of paid vacation per year.\n",
    "2. Carry Over: Up to 5 unused days can be carried to the next year.\n",
    "3. Request Process: Submit vacation requests at least 2 weeks in advance.\n",
    "4. Approval: Manager approval required for all vacation requests.\n",
    "5. Blackout Dates: December 15-31 are company blackout dates.\n",
    "\"\"\"\n",
    "\n",
    "document_2 = \"\"\"\n",
    "Document: Company_Policy_2025.pdf\n",
    "\n",
    "Employee Vacation Policy\n",
    "\n",
    "1. Annual Leave: Employees are entitled to 25 days of paid vacation per year.\n",
    "2. Carry Over: Up to 10 unused days can be carried to the next year.\n",
    "3. Request Process: Submit vacation requests at least 1 week in advance.\n",
    "4. Approval: Manager approval required for requests over 5 consecutive days.\n",
    "5. Flexible Holidays: Employees can choose 3 floating holidays.\n",
    "\"\"\"\n",
    "\n",
    "# Combine documents for comparison\n",
    "combined_docs = f\"{document_1}\\n\\n---\\n\\n{document_2}\"\n",
    "\n",
    "print(\"Documents prepared for comparison\")\n",
    "print(f\"Total length: {len(combined_docs)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the documents\n",
    "comparison_df = comparator.compare_documents(combined_docs)\n",
    "\n",
    "print(\"\\nComparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "display(comparison_df)  # Use display() in Jupyter for better formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the comparison results\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(f\"Columns: {list(comparison_df.columns)}\")\n",
    "print(f\"Rows: {len(comparison_df)}\")\n",
    "\n",
    "# Show detailed view of each row\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"\\n--- Row {idx} ---\")\n",
    "    for col in comparison_df.columns:\n",
    "        print(f\"{col}: {row[col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252052c9",
   "metadata": {},
   "source": [
    "## 6. Working with PDF Files\n",
    "\n",
    "Example of comparing actual PDF documents using the DocumentComparator from data_ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec045d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document_ingestion.data_ingestion import DocumentComparator\n",
    "\n",
    "# Initialize document handler for PDF operations\n",
    "doc_handler = DocumentComparator(session_id=\"comparison_demo\")\n",
    "print(f\"DocumentComparator initialized with session: {doc_handler.session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab19f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare PDF files (update paths as needed)\n",
    "import os\n",
    "\n",
    "PDF_DIR = \"data/document_compare/comparison_demo\"  # Session directory\n",
    "\n",
    "if os.path.exists(PDF_DIR) and os.listdir(PDF_DIR):\n",
    "    # Combine all PDFs in the session directory\n",
    "    combined_text = doc_handler.combine_documents()\n",
    "    \n",
    "    print(f\"Combined document length: {len(combined_text)} characters\")\n",
    "    \n",
    "    # Compare using LLM\n",
    "    pdf_comparison = comparator.compare_documents(combined_text)\n",
    "    print(\"\\nPDF Comparison Results:\")\n",
    "    display(pdf_comparison)\n",
    "else:\n",
    "    print(f\"No PDFs found in: {PDF_DIR}\")\n",
    "    print(\"To test PDF comparison:\")\n",
    "    print(\"1. Upload PDFs using doc_handler.save_uploaded_files()\")\n",
    "    print(\"2. Then run this cell again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e1f89",
   "metadata": {},
   "source": [
    "## 7. Export Comparison Results\n",
    "\n",
    "Export comparison results to various formats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_path = \"data/document_compare/comparison_results.csv\"\n",
    "comparison_df.to_csv(output_path, index=False)\n",
    "print(f\"Results exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "json_output = comparison_df.to_json(orient='records', indent=2)\n",
    "print(\"JSON Output:\")\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26eb51",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. **DocumentComparatorLLM** uses LLM to intelligently compare documents\n",
    "2. The LCEL chain (`prompt | llm | parser`) processes combined document text\n",
    "3. Results are returned as a **pandas DataFrame** for easy analysis\n",
    "4. Supports both text and PDF document comparison\n",
    "\n",
    "### Possible Extensions\n",
    "- Add visualization of comparison results (charts, diff views)\n",
    "- Support for more than 2 documents in comparison\n",
    "- Implement version-specific comparisons (track changes over time)\n",
    "- Add semantic similarity scores between document sections\n",
    "- Export comparison reports in PDF/HTML format"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
