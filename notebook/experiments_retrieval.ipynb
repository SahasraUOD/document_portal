{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04d797c",
   "metadata": {},
   "source": [
    "# Experiments: retrieval\n",
    "\n",
    "**Original File:** `src/document_chat/retrieval.py`\n",
    "\n",
    "## Purpose\n",
    "This module implements a Conversational RAG (Retrieval-Augmented Generation) system using LangChain Expression Language (LCEL). It enables context-aware question answering over documents stored in a FAISS vector index, with support for maintaining chat history.\n",
    "\n",
    "## Key Components\n",
    "- **ConversationalRAG class**: Main RAG pipeline with lazy retriever initialization\n",
    "  - `load_retriever_from_faiss()`: Load FAISS vectorstore and build retriever\n",
    "  - `invoke()`: Execute the RAG pipeline with user input and chat history\n",
    "  - `_build_lcel_chain()`: Build the LCEL processing chain\n",
    "\n",
    "## Prerequisites\n",
    "- `langchain`, `langchain-core`, `langchain-community` installed\n",
    "- FAISS vector index created (see `data_ingestion.py`)\n",
    "- Environment variables configured (API keys for LLM)\n",
    "- Prompt templates in `prompt/prompt_library.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941e7e3",
   "metadata": {},
   "source": [
    "## Instructions & Setup Guide\n",
    "\n",
    "### Execution Order\n",
    "1. Run the imports cell\n",
    "2. Review the ConversationalRAG class definition\n",
    "3. Initialize the RAG system with a session ID\n",
    "4. Load a FAISS index using `load_retriever_from_faiss()`\n",
    "5. Invoke the RAG with questions and chat history\n",
    "\n",
    "### Dependencies\n",
    "```bash\n",
    "pip install langchain langchain-core langchain-community faiss-cpu python-dotenv\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "- Ensure `.env` file contains `GROQ_API_KEY` and/or `GOOGLE_API_KEY`\n",
    "- FAISS index must exist at the specified path\n",
    "- Run from project root directory for proper imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb6ec8",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies\n",
    "\n",
    "Import all required modules for the Conversational RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from prompt.prompt_library import PROMPT_REGISTRY\n",
    "from model.models import PromptType\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd14dc4",
   "metadata": {},
   "source": [
    "## 2. ConversationalRAG Class Definition\n",
    "\n",
    "The main class that orchestrates the RAG pipeline. Key features:\n",
    "- **Lazy initialization**: Retriever and chain are built only when needed\n",
    "- **LCEL-based**: Uses LangChain Expression Language for composable chains\n",
    "- **Context-aware**: Rewrites questions based on chat history before retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c75faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalRAG:\n",
    "    \"\"\"\n",
    "    LCEL-based Conversational RAG with lazy retriever initialization.\n",
    "\n",
    "    Usage:\n",
    "        rag = ConversationalRAG(session_id=\"abc\")\n",
    "        rag.load_retriever_from_faiss(index_path=\"faiss_index/abc\", k=5, index_name=\"index\")\n",
    "        answer = rag.invoke(\"What is ...?\", chat_history=[])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, session_id: Optional[str], retriever=None):\n",
    "        try:\n",
    "            self.session_id = session_id\n",
    "\n",
    "            # Load LLM and prompts once\n",
    "            self.llm = self._load_llm()\n",
    "            self.contextualize_prompt: ChatPromptTemplate = PROMPT_REGISTRY[\n",
    "                PromptType.CONTEXTUALIZE_QUESTION.value\n",
    "            ]\n",
    "            self.qa_prompt: ChatPromptTemplate = PROMPT_REGISTRY[\n",
    "                PromptType.CONTEXT_QA.value\n",
    "            ]\n",
    "\n",
    "            # Lazy pieces\n",
    "            self.retriever = retriever\n",
    "            self.chain = None\n",
    "            if self.retriever is not None:\n",
    "                self._build_lcel_chain()\n",
    "\n",
    "            log.info(\"ConversationalRAG initialized\", session_id=self.session_id)\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to initialize ConversationalRAG\", error=str(e))\n",
    "            raise DocumentPortalException(\"Initialization error in ConversationalRAG\", sys)\n",
    "\n",
    "    # ---------- Private Methods ----------\n",
    "\n",
    "    def _load_llm(self):\n",
    "        \"\"\"Load the LLM from ModelLoader.\"\"\"\n",
    "        try:\n",
    "            llm = ModelLoader().load_llm()\n",
    "            if not llm:\n",
    "                raise ValueError(\"LLM could not be loaded\")\n",
    "            log.info(\"LLM loaded successfully\", session_id=self.session_id)\n",
    "            return llm\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to load LLM\", error=str(e))\n",
    "            raise DocumentPortalException(\"LLM loading error in ConversationalRAG\", sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_docs(docs) -> str:\n",
    "        \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "        return \"\\n\\n\".join(getattr(d, \"page_content\", str(d)) for d in docs)\n",
    "\n",
    "    def _build_lcel_chain(self):\n",
    "        \"\"\"Build the LCEL chain for conversational RAG.\"\"\"\n",
    "        try:\n",
    "            if self.retriever is None:\n",
    "                raise DocumentPortalException(\"No retriever set before building chain\", sys)\n",
    "\n",
    "            # Step 1: Rewrite user question with chat history context\n",
    "            question_rewriter = (\n",
    "                {\"input\": itemgetter(\"input\"), \"chat_history\": itemgetter(\"chat_history\")}\n",
    "                | self.contextualize_prompt\n",
    "                | self.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # Step 2: Retrieve docs for rewritten question\n",
    "            retrieve_docs = question_rewriter | self.retriever | self._format_docs\n",
    "\n",
    "            # Step 3: Answer using retrieved context + original input + chat history\n",
    "            self.chain = (\n",
    "                {\n",
    "                    \"context\": retrieve_docs,\n",
    "                    \"input\": itemgetter(\"input\"),\n",
    "                    \"chat_history\": itemgetter(\"chat_history\"),\n",
    "                }\n",
    "                | self.qa_prompt\n",
    "                | self.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            log.info(\"LCEL graph built successfully\", session_id=self.session_id)\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to build LCEL chain\", error=str(e), session_id=self.session_id)\n",
    "            raise DocumentPortalException(\"Failed to build LCEL chain\", sys)\n",
    "\n",
    "print(\"ConversationalRAG class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e938d91",
   "metadata": {},
   "source": [
    "## 3. Public API Methods\n",
    "\n",
    "Add the public methods to the ConversationalRAG class:\n",
    "- `load_retriever_from_faiss()`: Load FAISS index and create retriever\n",
    "- `invoke()`: Execute the RAG chain with user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these methods to ConversationalRAG class\n",
    "\n",
    "def load_retriever_from_faiss(\n",
    "    self,\n",
    "    index_path: str,\n",
    "    k: int = 5,\n",
    "    index_name: str = \"index\",\n",
    "    search_type: str = \"similarity\",\n",
    "    search_kwargs: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load FAISS vectorstore from disk and build retriever + LCEL chain.\n",
    "    \n",
    "    Args:\n",
    "        index_path: Path to FAISS index directory\n",
    "        k: Number of documents to retrieve (default: 5)\n",
    "        index_name: Name of the index files (default: \"index\")\n",
    "        search_type: Type of search - \"similarity\" or \"mmr\" (default: \"similarity\")\n",
    "        search_kwargs: Additional search parameters\n",
    "    \n",
    "    Returns:\n",
    "        The configured retriever\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.isdir(index_path):\n",
    "            raise FileNotFoundError(f\"FAISS index directory not found: {index_path}\")\n",
    "\n",
    "        embeddings = ModelLoader().load_embeddings()\n",
    "        vectorstore = FAISS.load_local(\n",
    "            index_path,\n",
    "            embeddings,\n",
    "            index_name=index_name,\n",
    "            allow_dangerous_deserialization=True,  # ok if you trust the index\n",
    "        )\n",
    "\n",
    "        if search_kwargs is None:\n",
    "            search_kwargs = {\"k\": k}\n",
    "\n",
    "        self.retriever = vectorstore.as_retriever(\n",
    "            search_type=search_type, search_kwargs=search_kwargs\n",
    "        )\n",
    "        self._build_lcel_chain()\n",
    "\n",
    "        log.info(\n",
    "            \"FAISS retriever loaded successfully\",\n",
    "            index_path=index_path,\n",
    "            index_name=index_name,\n",
    "            k=k,\n",
    "            session_id=self.session_id,\n",
    "        )\n",
    "        return self.retriever\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(\"Failed to load retriever from FAISS\", error=str(e))\n",
    "        raise DocumentPortalException(\"Loading error in ConversationalRAG\", sys)\n",
    "\n",
    "# Attach to class\n",
    "ConversationalRAG.load_retriever_from_faiss = load_retriever_from_faiss\n",
    "print(\"load_retriever_from_faiss method added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(self, user_input: str, chat_history: Optional[List[BaseMessage]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Invoke the LCEL pipeline with user input and chat history.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user's question\n",
    "        chat_history: List of previous messages (HumanMessage/AIMessage)\n",
    "    \n",
    "    Returns:\n",
    "        The generated answer string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if self.chain is None:\n",
    "            raise DocumentPortalException(\n",
    "                \"RAG chain not initialized. Call load_retriever_from_faiss() before invoke().\", sys\n",
    "            )\n",
    "        chat_history = chat_history or []\n",
    "        payload = {\"input\": user_input, \"chat_history\": chat_history}\n",
    "        answer = self.chain.invoke(payload)\n",
    "        if not answer:\n",
    "            log.warning(\n",
    "                \"No answer generated\", user_input=user_input, session_id=self.session_id\n",
    "            )\n",
    "            return \"no answer generated.\"\n",
    "        log.info(\n",
    "            \"Chain invoked successfully\",\n",
    "            session_id=self.session_id,\n",
    "            user_input=user_input,\n",
    "            answer_preview=str(answer)[:150],\n",
    "        )\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        log.error(\"Failed to invoke ConversationalRAG\", error=str(e))\n",
    "        raise DocumentPortalException(\"Invocation error in ConversationalRAG\", sys)\n",
    "\n",
    "# Attach to class\n",
    "ConversationalRAG.invoke = invoke\n",
    "print(\"invoke method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38a160",
   "metadata": {},
   "source": [
    "## 4. Usage Example\n",
    "\n",
    "Demonstrate how to use the ConversationalRAG class with a FAISS index.\n",
    "\n",
    "**Note:** You need to have a FAISS index created first. Use `data_ingestion.py` to create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - adjust paths as needed\n",
    "\n",
    "# Initialize the RAG system\n",
    "session_id = \"demo_session\"\n",
    "rag = ConversationalRAG(session_id=session_id)\n",
    "print(f\"RAG initialized with session: {session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e523faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index (update path to your actual index)\n",
    "INDEX_PATH = \"faiss_index/demo_session\"  # Update this path\n",
    "\n",
    "# Check if index exists before loading\n",
    "if os.path.exists(INDEX_PATH):\n",
    "    retriever = rag.load_retriever_from_faiss(\n",
    "        index_path=INDEX_PATH,\n",
    "        k=5,  # Number of documents to retrieve\n",
    "        search_type=\"similarity\"\n",
    "    )\n",
    "    print(f\"Retriever loaded from: {INDEX_PATH}\")\n",
    "else:\n",
    "    print(f\"Index not found at: {INDEX_PATH}\")\n",
    "    print(\"Please create an index first using data_ingestion.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question (only if retriever was loaded)\n",
    "if rag.retriever is not None:\n",
    "    question = \"What is the main topic of the document?\"\n",
    "    chat_history = []  # Empty for first question\n",
    "    \n",
    "    answer = rag.invoke(question, chat_history=chat_history)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6817235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question with chat history\n",
    "if rag.retriever is not None:\n",
    "    # Build chat history from previous interaction\n",
    "    chat_history = [\n",
    "        HumanMessage(content=\"What is the main topic of the document?\"),\n",
    "        AIMessage(content=answer)  # Previous answer\n",
    "    ]\n",
    "    \n",
    "    follow_up = \"Can you elaborate on that?\"\n",
    "    follow_up_answer = rag.invoke(follow_up, chat_history=chat_history)\n",
    "    \n",
    "    print(f\"Follow-up: {follow_up}\")\n",
    "    print(f\"Answer: {follow_up_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfffda7",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. **ConversationalRAG** provides a complete RAG pipeline with chat history support\n",
    "2. The LCEL chain consists of three steps:\n",
    "   - Question rewriting (contextualizing with chat history)\n",
    "   - Document retrieval from FAISS\n",
    "   - Answer generation with retrieved context\n",
    "3. Lazy initialization allows flexible setup of the retriever\n",
    "\n",
    "### Possible Extensions\n",
    "- Add streaming support for real-time responses\n",
    "- Implement different retrieval strategies (MMR, hybrid search)\n",
    "- Add source document citations to answers\n",
    "- Implement conversation memory persistence\n",
    "- Add re-ranking of retrieved documents"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
