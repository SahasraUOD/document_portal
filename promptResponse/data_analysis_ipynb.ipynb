{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a38110",
   "metadata": {},
   "source": [
    "# Document Analyzer - Step-by-Step Notebook\n",
    "\n",
    "This notebook converts `src/document_analyzer/data_analysis.py` into a procedural, debuggable format.\n",
    "\n",
    "## What This Does\n",
    "Analyzes documents using an LLM to extract **structured metadata**:\n",
    "- Title, Author, Publisher\n",
    "- Date Created, Last Modified\n",
    "- Language, Page Count\n",
    "- Summary (list of key points)\n",
    "- Sentiment/Tone\n",
    "\n",
    "## Prerequisites\n",
    "- API keys set: `GOOGLE_API_KEY` and/or `GROQ_API_KEY`\n",
    "- Config file at `config/config.yaml`\n",
    "- A document (text) to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc6fec",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Configuration Placeholders\n",
    "\n",
    "**Purpose:** Define sample document text for testing. Replace with your actual document content.\n",
    "\n",
    "**External Dependencies:**\n",
    "- Environment variables: `GOOGLE_API_KEY`, `GROQ_API_KEY`, `LLM_PROVIDER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION PLACEHOLDERS - MODIFY THESE BEFORE RUNNING\n",
    "# ============================================================\n",
    "\n",
    "# Sample document text for testing (replace with your actual document)\n",
    "SAMPLE_DOCUMENT_TEXT = \"\"\"\n",
    "Annual Report 2025\n",
    "By: John Smith, Jane Doe\n",
    "Published by: Acme Corporation\n",
    "Date: January 15, 2025\n",
    "\n",
    "Executive Summary:\n",
    "This report presents the financial performance and strategic initiatives \n",
    "of Acme Corporation for the fiscal year 2025. Key highlights include \n",
    "a 15% increase in revenue, expansion into three new markets, and the \n",
    "successful launch of our flagship product line.\n",
    "\n",
    "The company demonstrated strong growth despite challenging market conditions.\n",
    "Our commitment to innovation and customer satisfaction continues to drive \n",
    "our success in the competitive landscape.\n",
    "\n",
    "Looking ahead, we plan to invest heavily in R&D and sustainable practices\n",
    "to ensure long-term value creation for our stakeholders.\n",
    "\"\"\"  # <-- REPLACE WITH YOUR DOCUMENT TEXT\n",
    "\n",
    "# Or load from a file:\n",
    "# DOCUMENT_FILE_PATH = \"path/to/your/document.txt\"  # <-- UNCOMMENT AND SET\n",
    "\n",
    "print(f\"Document preview ({len(SAMPLE_DOCUMENT_TEXT)} chars):\")\n",
    "print(SAMPLE_DOCUMENT_TEXT[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc85ee9",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Imports\n",
    "\n",
    "**Purpose:** Import all required libraries and modules.\n",
    "\n",
    "**Key Dependencies:**\n",
    "- `langchain_core.output_parsers.JsonOutputParser`: Parses LLM output as JSON\n",
    "- `langchain.output_parsers.OutputFixingParser`: Auto-fixes malformed JSON responses\n",
    "- `utils.model_loader.ModelLoader`: Loads LLM from config\n",
    "- `model.models.Metadata`: Pydantic model defining expected output structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd71190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Union\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from model.models import Metadata\n",
    "from prompt.prompt_library import PROMPT_REGISTRY\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e21d4",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Understanding the Metadata Schema\n",
    "\n",
    "**Purpose:** Show the expected output structure from the document analyzer.\n",
    "\n",
    "The `Metadata` Pydantic model defines what fields the LLM should extract:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `Summary` | `List[str]` | Key points from the document |\n",
    "| `Title` | `str` | Document title |\n",
    "| `Author` | `List[str]` | Author names |\n",
    "| `DateCreated` | `str` | Creation date |\n",
    "| `LastModifiedDate` | `str` | Last modification date |\n",
    "| `Publisher` | `str` | Publisher name |\n",
    "| `Language` | `str` | Document language |\n",
    "| `PageCount` | `int \\| str` | Number of pages (or \"Not Available\") |\n",
    "| `SentimentTone` | `str` | Overall tone/sentiment |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d543617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Metadata schema\n",
    "print(\"Metadata Schema:\")\n",
    "print(\"=\"*60)\n",
    "for field_name, field_info in Metadata.model_fields.items():\n",
    "    print(f\"  {field_name}: {field_info.annotation}\")\n",
    "\n",
    "print(\"\\nExample output:\")\n",
    "example = Metadata(\n",
    "    Summary=[\"Key point 1\", \"Key point 2\"],\n",
    "    Title=\"Sample Document\",\n",
    "    Author=[\"John Doe\"],\n",
    "    DateCreated=\"2025-01-01\",\n",
    "    LastModifiedDate=\"2025-01-15\",\n",
    "    Publisher=\"Example Corp\",\n",
    "    Language=\"English\",\n",
    "    PageCount=10,\n",
    "    SentimentTone=\"Professional\"\n",
    ")\n",
    "print(example.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88e377",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Load LLM Model\n",
    "\n",
    "**Purpose:** Initialize the Language Model that will analyze documents.\n",
    "\n",
    "**What happens:**\n",
    "- `ModelLoader` reads `config/config.yaml` for model settings\n",
    "- Checks `LLM_PROVIDER` env var (default: \"google\")\n",
    "- Returns either `ChatGoogleGenerativeAI` or `ChatGroq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af675768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM\n",
    "try:\n",
    "    loader = ModelLoader()\n",
    "    llm = loader.load_llm()\n",
    "    \n",
    "    if not llm:\n",
    "        raise ValueError(\"LLM could not be loaded\")\n",
    "    \n",
    "    log.info(\"LLM loaded successfully\")\n",
    "    print(f\"LLM loaded: {type(llm).__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading LLM: {e}\")\n",
    "    raise DocumentPortalException(\"LLM loading error\", sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ac971",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Initialize Output Parsers\n",
    "\n",
    "**Purpose:** Set up parsers to convert LLM text output into structured JSON.\n",
    "\n",
    "**Two parsers work together:**\n",
    "\n",
    "1. **JsonOutputParser**: Primary parser that expects the LLM to return JSON matching `Metadata` schema\n",
    "2. **OutputFixingParser**: Backup parser that auto-fixes malformed JSON by asking the LLM to correct it\n",
    "\n",
    "**Why both?** LLMs sometimes return slightly malformed JSON (missing quotes, trailing commas). The fixing parser catches and repairs these errors automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f83ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parsers\n",
    "parser = JsonOutputParser(pydantic_object=Metadata)\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
    "\n",
    "print(\"Parsers initialized:\")\n",
    "print(f\"  - JsonOutputParser (expects Metadata schema)\")\n",
    "print(f\"  - OutputFixingParser (auto-fixes malformed JSON)\")\n",
    "\n",
    "# Show the format instructions that will be sent to the LLM\n",
    "print(\"\\nFormat instructions for LLM:\")\n",
    "print(\"=\"*60)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15e79d",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Load Document Analysis Prompt\n",
    "\n",
    "**Purpose:** Load the prompt template that tells the LLM how to analyze documents.\n",
    "\n",
    "**The prompt includes:**\n",
    "- Instructions for document analysis\n",
    "- Required output format (JSON schema)\n",
    "- Placeholder for the document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc696ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document analysis prompt\n",
    "prompt = PROMPT_REGISTRY[\"document_analysis\"]\n",
    "\n",
    "print(\"Document Analysis Prompt loaded!\")\n",
    "print(\"\\nPrompt template:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24870a43",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Build the Analysis Chain\n",
    "\n",
    "**Purpose:** Create the LCEL chain that connects prompt → LLM → parser.\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Document Text + Format Instructions\n",
    "    ↓\n",
    "Prompt Template (fills placeholders)\n",
    "    ↓\n",
    "LLM (generates JSON response)\n",
    "    ↓\n",
    "OutputFixingParser (parses & fixes JSON)\n",
    "    ↓\n",
    "Python Dict (Metadata fields)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the analysis chain\n",
    "analysis_chain = prompt | llm | fixing_parser\n",
    "\n",
    "log.info(\"Document analysis chain initialized\")\n",
    "print(\"Analysis chain built successfully!\")\n",
    "print(\"\\nChain structure:\")\n",
    "print(\"  prompt → llm → fixing_parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ea257",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Helper Function - Analyze Document\n",
    "\n",
    "**Purpose:** Wrapper function to invoke the analysis chain with proper error handling.\n",
    "\n",
    "**Parameters:**\n",
    "- `document_text`: The full text content of the document to analyze\n",
    "\n",
    "**Returns:** Dictionary with extracted metadata fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(document_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze a document's text and extract structured metadata & summary.\n",
    "    \n",
    "    Args:\n",
    "        document_text: The full text content of the document\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extracted metadata with keys:\n",
    "            - Summary, Title, Author, DateCreated, LastModifiedDate,\n",
    "            - Publisher, Language, PageCount, SentimentTone\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log.info(\"Starting document analysis\", doc_length=len(document_text))\n",
    "        \n",
    "        response = analysis_chain.invoke({\n",
    "            \"format_instructions\": parser.get_format_instructions(),\n",
    "            \"document_text\": document_text\n",
    "        })\n",
    "        \n",
    "        log.info(\"Metadata extraction successful\", keys=list(response.keys()))\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during analysis: {e}\")\n",
    "        log.error(\"Metadata analysis failed\", error=str(e))\n",
    "        raise DocumentPortalException(\"Metadata extraction failed\", sys)\n",
    "\n",
    "print(\"analyze_document function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c455d",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Test - Analyze Sample Document\n",
    "\n",
    "**Purpose:** Test the analysis chain with the sample document from Cell 1.\n",
    "\n",
    "**Expected output:** A dictionary containing all Metadata fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: Analyze Sample Document\n",
    "# ============================================================\n",
    "\n",
    "print(\"Analyzing document...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = analyze_document(SAMPLE_DOCUMENT_TEXT)\n",
    "\n",
    "print(\"\\nExtracted Metadata:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in result.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"\\n{key}:\")\n",
    "        for item in value:\n",
    "            print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1632614",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Test - Load and Analyze from File\n",
    "\n",
    "**Purpose:** Analyze a document loaded from a file path.\n",
    "\n",
    "**Modify `DOCUMENT_FILE_PATH` to point to your actual document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95784177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: Load and Analyze from File\n",
    "# ============================================================\n",
    "\n",
    "DOCUMENT_FILE_PATH = \"data/multi_doc_chat/state_of_the_union.txt\"  # <-- CHANGE THIS\n",
    "\n",
    "try:\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(DOCUMENT_FILE_PATH):\n",
    "        print(f\"File not found: {DOCUMENT_FILE_PATH}\")\n",
    "        print(\"Skipping file-based test. Update DOCUMENT_FILE_PATH to test.\")\n",
    "    else:\n",
    "        # Load document text from file\n",
    "        with open(DOCUMENT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "        \n",
    "        print(f\"Loaded document: {DOCUMENT_FILE_PATH}\")\n",
    "        print(f\"Document length: {len(file_content)} characters\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Analyze (truncate if too long for demo)\n",
    "        content_to_analyze = file_content[:10000]  # First 10K chars for testing\n",
    "        if len(file_content) > 10000:\n",
    "            print(f\"Note: Analyzing first 10,000 characters only\")\n",
    "        \n",
    "        file_result = analyze_document(content_to_analyze)\n",
    "        \n",
    "        print(\"\\nExtracted Metadata:\")\n",
    "        print(\"=\"*60)\n",
    "        for key, value in file_result.items():\n",
    "            if isinstance(value, list):\n",
    "                print(f\"\\n{key}:\")\n",
    "                for item in value:\n",
    "                    print(f\"  - {item}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b123de",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Debug - Inspect Raw LLM Response\n",
    "\n",
    "**Purpose:** See the raw LLM output before parsing, useful for debugging JSON issues.\n",
    "\n",
    "This bypasses the fixing parser to show exactly what the LLM returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEBUG: Inspect Raw LLM Response\n",
    "# ============================================================\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chain without fixing parser (raw output)\n",
    "raw_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Getting raw LLM response (before parsing)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "raw_response = raw_chain.invoke({\n",
    "    \"format_instructions\": parser.get_format_instructions(),\n",
    "    \"document_text\": SAMPLE_DOCUMENT_TEXT[:500]  # Use short text for debug\n",
    "})\n",
    "\n",
    "print(\"\\nRaw LLM Response:\")\n",
    "print(\"-\"*60)\n",
    "print(raw_response)\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Try parsing manually\n",
    "print(\"\\nAttempting to parse...\")\n",
    "try:\n",
    "    import json\n",
    "    parsed = json.loads(raw_response)\n",
    "    print(\"✓ Valid JSON!\")\n",
    "    print(f\"Keys: {list(parsed.keys())}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"✗ Invalid JSON: {e}\")\n",
    "    print(\"The OutputFixingParser would attempt to fix this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f429a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Debug - Test with Different Document Types\n",
    "\n",
    "**Purpose:** Test the analyzer with different types of documents to see how it handles various content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEBUG: Test Different Document Types\n",
    "# ============================================================\n",
    "\n",
    "test_documents = {\n",
    "    \"Technical Doc\": \"\"\"\n",
    "        Technical Specification v2.1\n",
    "        Author: Engineering Team\n",
    "        Last Updated: 2025-02-01\n",
    "        \n",
    "        This document outlines the API specifications for the \n",
    "        Document Portal system. Key endpoints include /analyze, \n",
    "        /compare, and /chat. Authentication uses JWT tokens.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"News Article\": \"\"\"\n",
    "        Breaking: Tech Company Announces Record Profits\n",
    "        By Sarah Johnson, Tech Times\n",
    "        February 5, 2025\n",
    "        \n",
    "        In a stunning quarterly report, the company exceeded \n",
    "        analyst expectations with 40% year-over-year growth.\n",
    "        CEO stated optimism about future AI investments.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Minimal Text\": \"Just a short note about meetings tomorrow.\"\n",
    "}\n",
    "\n",
    "for doc_type, content in test_documents.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {doc_type}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        result = analyze_document(content)\n",
    "        print(f\"Title: {result.get('Title', 'N/A')}\")\n",
    "        print(f\"Author: {result.get('Author', 'N/A')}\")\n",
    "        print(f\"Sentiment: {result.get('SentimentTone', 'N/A')}\")\n",
    "        print(f\"Summary: {result.get('Summary', ['N/A'])[0][:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088030f6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13: Export Results to JSON\n",
    "\n",
    "**Purpose:** Save the analysis results to a JSON file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPORT: Save Results to JSON\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_PATH = \"data/document_analysis/analysis_result.json\"  # <-- CHANGE THIS\n",
    "\n",
    "# Run analysis\n",
    "result = analyze_document(SAMPLE_DOCUMENT_TEXT)\n",
    "\n",
    "# Add metadata about the analysis\n",
    "output = {\n",
    "    \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "    \"document_length\": len(SAMPLE_DOCUMENT_TEXT),\n",
    "    \"results\": result\n",
    "}\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# Save to file\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to: {OUTPUT_PATH}\")\n",
    "print(\"\\nFile contents:\")\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07322063",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Variables Persisting Between Cells\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `llm` | LLM | Language model instance |\n",
    "| `parser` | JsonOutputParser | Parses LLM output to JSON |\n",
    "| `fixing_parser` | OutputFixingParser | Auto-fixes malformed JSON |\n",
    "| `prompt` | ChatPromptTemplate | Document analysis prompt template |\n",
    "| `analysis_chain` | Chain | Complete analysis pipeline |\n",
    "\n",
    "### Functions\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `analyze_document(text)` | Analyze document and return metadata dict |\n",
    "\n",
    "### Output Schema (Metadata)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Summary\": [\"Point 1\", \"Point 2\"],\n",
    "  \"Title\": \"Document Title\",\n",
    "  \"Author\": [\"Author Name\"],\n",
    "  \"DateCreated\": \"2025-01-01\",\n",
    "  \"LastModifiedDate\": \"2025-01-15\",\n",
    "  \"Publisher\": \"Publisher Name\",\n",
    "  \"Language\": \"English\",\n",
    "  \"PageCount\": 10,\n",
    "  \"SentimentTone\": \"Professional\"\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
