{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c20833",
   "metadata": {},
   "source": [
    "# Data Ingestion - Step-by-Step Notebook\n",
    "\n",
    "This notebook converts `src/document_ingestion/data_ingestion.py` into a procedural, debuggable format.\n",
    "\n",
    "## What This File Contains\n",
    "Four major components for document handling:\n",
    "\n",
    "| Class | Purpose |\n",
    "|-------|--------|\n",
    "| `FaissManager` | Manages FAISS vector index (load/create/add documents) |\n",
    "| `ChatIngestor` | Ingests documents for chat - splits, embeds, builds retriever |\n",
    "| `DocHandler` | Saves and reads PDFs for document analysis |\n",
    "| `DocumentComparator` | Saves, reads, and combines PDFs for comparison |\n",
    "\n",
    "## Prerequisites\n",
    "- API keys set: `GOOGLE_API_KEY` (for embeddings)\n",
    "- Config file at `config/config.yaml`\n",
    "- PyMuPDF (`fitz`) installed: `pip install pymupdf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58b4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Configuration Placeholders\n",
    "\n",
    "**Purpose:** Define all configurable paths and parameters.\n",
    "\n",
    "**External Dependencies:**\n",
    "- `DATA_STORAGE_PATH` env var (optional)\n",
    "- Directories will be created automatically if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8570d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION PLACEHOLDERS - MODIFY THESE BEFORE RUNNING\n",
    "# ============================================================\n",
    "\n",
    "# Base directories\n",
    "TEMP_BASE = \"data\"                          # Where uploaded files are temporarily stored\n",
    "FAISS_BASE = \"faiss_index\"                  # Where FAISS indexes are stored\n",
    "ANALYSIS_DIR = \"data/document_analysis\"     # For DocHandler\n",
    "COMPARE_DIR = \"data/document_compare\"       # For DocumentComparator\n",
    "\n",
    "# Session handling\n",
    "USE_SESSION_DIRS = True   # Create session-specific subdirectories\n",
    "SESSION_ID = None         # Auto-generate if None, or set specific ID like \"test_session_001\"\n",
    "\n",
    "# Chunking parameters (for ChatIngestor)\n",
    "CHUNK_SIZE = 1000         # Characters per chunk\n",
    "CHUNK_OVERLAP = 200       # Overlap between chunks\n",
    "RETRIEVER_K = 5           # Number of documents to retrieve\n",
    "\n",
    "# Supported file types\n",
    "SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  TEMP_BASE: {TEMP_BASE}\")\n",
    "print(f\"  FAISS_BASE: {FAISS_BASE}\")\n",
    "print(f\"  CHUNK_SIZE: {CHUNK_SIZE}, OVERLAP: {CHUNK_OVERLAP}\")\n",
    "print(f\"  RETRIEVER_K: {RETRIEVER_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd30ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Imports\n",
    "\n",
    "**Purpose:** Import all required libraries and modules.\n",
    "\n",
    "**Key Dependencies:**\n",
    "- `fitz` (PyMuPDF): For reading PDF files\n",
    "- `langchain`: Document schemas, text splitters, FAISS vectorstore\n",
    "- `utils.model_loader.ModelLoader`: Loads embedding model\n",
    "- `utils.file_io`: Session ID generation, file saving utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255dbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Dict, Any\n",
    "\n",
    "# PDF handling\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from utils.file_io import generate_session_id, save_uploaded_files\n",
    "from utils.document_ops import load_documents, concat_for_analysis, concat_for_comparison\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030da45",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Initialize Model Loader & Embeddings\n",
    "\n",
    "**Purpose:** Load the embedding model that will be used to convert text into vectors.\n",
    "\n",
    "**What happens:**\n",
    "- `ModelLoader` reads `config/config.yaml`\n",
    "- Loads Google Generative AI Embeddings (or configured alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f37571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model loader and embeddings\n",
    "try:\n",
    "    model_loader = ModelLoader()\n",
    "    embeddings = model_loader.load_embeddings()\n",
    "    \n",
    "    log.info(\"Embeddings loaded successfully\")\n",
    "    print(f\"Embeddings loaded: {type(embeddings).__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading embeddings: {e}\")\n",
    "    raise DocumentPortalException(\"Failed to load embeddings\", sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284cdeb",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Generate Session ID\n",
    "\n",
    "**Purpose:** Create or use a session identifier for organizing files.\n",
    "\n",
    "Sessions help:\n",
    "- Isolate different users/conversations\n",
    "- Prevent file conflicts\n",
    "- Enable easy cleanup of old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate or use provided session ID\n",
    "session_id = SESSION_ID or generate_session_id()\n",
    "\n",
    "print(f\"Session ID: {session_id}\")\n",
    "print(f\"This will be used to organize files in subdirectories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb6b61",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: FAISS Manager Functions\n",
    "\n",
    "The FAISS Manager handles:\n",
    "- Creating new FAISS indexes\n",
    "- Loading existing indexes from disk\n",
    "- Adding documents idempotently (no duplicates)\n",
    "- Tracking ingested documents via metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c47d44",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: FAISS Helper - Fingerprint Function\n",
    "\n",
    "**Purpose:** Generate a unique identifier for each document chunk to prevent duplicates.\n",
    "\n",
    "Uses either:\n",
    "- Source file path + row ID (if available)\n",
    "- SHA256 hash of content (fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprint(text: str, metadata: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Generate unique fingerprint for a document chunk.\n",
    "    Used to detect and skip duplicates during ingestion.\n",
    "    \n",
    "    Args:\n",
    "        text: Document content\n",
    "        metadata: Document metadata (may contain source, file_path, row_id)\n",
    "        \n",
    "    Returns:\n",
    "        str: Unique identifier for this chunk\n",
    "    \"\"\"\n",
    "    src = metadata.get(\"source\") or metadata.get(\"file_path\")\n",
    "    rid = metadata.get(\"row_id\")\n",
    "    if src is not None:\n",
    "        return f\"{src}::{'' if rid is None else rid}\"\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Test fingerprint\n",
    "print(\"Testing fingerprint function:\")\n",
    "print(f\"  With source: {fingerprint('content', {'source': 'doc.pdf', 'row_id': 1})}\")\n",
    "print(f\"  Without source: {fingerprint('unique content', {})[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8009351",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: FAISS Manager - Setup Variables\n",
    "\n",
    "**Purpose:** Initialize variables for managing the FAISS index.\n",
    "\n",
    "**Key components:**\n",
    "- `index_dir`: Where FAISS files are stored\n",
    "- `meta_path`: JSON file tracking ingested documents\n",
    "- `vs`: The FAISS vectorstore instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS Manager setup\n",
    "faiss_index_dir = Path(FAISS_BASE) / session_id if USE_SESSION_DIRS else Path(FAISS_BASE)\n",
    "faiss_index_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "meta_path = faiss_index_dir / \"ingested_meta.json\"\n",
    "faiss_meta: Dict[str, Any] = {\"rows\": {}}\n",
    "\n",
    "# Load existing metadata if present\n",
    "if meta_path.exists():\n",
    "    try:\n",
    "        faiss_meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) or {\"rows\": {}}\n",
    "        print(f\"Loaded existing metadata: {len(faiss_meta['rows'])} documents tracked\")\n",
    "    except Exception:\n",
    "        faiss_meta = {\"rows\": {}}\n",
    "        print(\"Could not load metadata, starting fresh\")\n",
    "else:\n",
    "    print(\"No existing metadata found, starting fresh\")\n",
    "\n",
    "# Vectorstore placeholder\n",
    "vectorstore: Optional[FAISS] = None\n",
    "\n",
    "print(f\"\\nFAISS index directory: {faiss_index_dir}\")\n",
    "print(f\"Metadata path: {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccc2df",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: FAISS Helper Functions\n",
    "\n",
    "**Purpose:** Define helper functions for FAISS operations.\n",
    "\n",
    "Functions:\n",
    "- `faiss_exists()`: Check if index files exist on disk\n",
    "- `save_meta()`: Persist metadata to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c556314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_exists() -> bool:\n",
    "    \"\"\"Check if FAISS index files exist on disk.\"\"\"\n",
    "    return (faiss_index_dir / \"index.faiss\").exists() and (faiss_index_dir / \"index.pkl\").exists()\n",
    "\n",
    "def save_meta():\n",
    "    \"\"\"Save metadata to JSON file.\"\"\"\n",
    "    meta_path.write_text(json.dumps(faiss_meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"FAISS index exists: {faiss_exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cd2f3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: FAISS - Load or Create Index\n",
    "\n",
    "**Purpose:** Load existing FAISS index from disk, or create a new one.\n",
    "\n",
    "**Logic:**\n",
    "1. If index exists on disk → load it\n",
    "2. If no index and texts provided → create new index\n",
    "3. If no index and no texts → raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_faiss(texts: Optional[List[str]] = None, \n",
    "                          metadatas: Optional[List[dict]] = None) -> FAISS:\n",
    "    \"\"\"\n",
    "    Load existing FAISS index or create new one from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to index (required if creating new)\n",
    "        metadatas: Optional metadata for each text\n",
    "        \n",
    "    Returns:\n",
    "        FAISS: The vectorstore instance\n",
    "    \"\"\"\n",
    "    global vectorstore\n",
    "    \n",
    "    # Try to load existing index\n",
    "    if faiss_exists():\n",
    "        vectorstore = FAISS.load_local(\n",
    "            str(faiss_index_dir),\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "        log.info(\"FAISS index loaded from disk\", path=str(faiss_index_dir))\n",
    "        print(f\"Loaded existing FAISS index from: {faiss_index_dir}\")\n",
    "        return vectorstore\n",
    "    \n",
    "    # Create new index if texts provided\n",
    "    if not texts:\n",
    "        raise DocumentPortalException(\"No existing FAISS index and no data to create one\", sys)\n",
    "    \n",
    "    vectorstore = FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas or [])\n",
    "    vectorstore.save_local(str(faiss_index_dir))\n",
    "    log.info(\"New FAISS index created\", path=str(faiss_index_dir), docs=len(texts))\n",
    "    print(f\"Created new FAISS index at: {faiss_index_dir} ({len(texts)} documents)\")\n",
    "    return vectorstore\n",
    "\n",
    "print(\"load_or_create_faiss function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185a1c5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: FAISS - Add Documents Idempotently\n",
    "\n",
    "**Purpose:** Add new documents to FAISS index, skipping duplicates.\n",
    "\n",
    "**Idempotent behavior:**\n",
    "- Uses fingerprints to detect already-ingested documents\n",
    "- Only adds truly new documents\n",
    "- Updates metadata tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ac47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_faiss(docs: List[Document]) -> int:\n",
    "    \"\"\"\n",
    "    Add documents to FAISS index, skipping duplicates.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of LangChain Document objects\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of new documents actually added\n",
    "    \"\"\"\n",
    "    global faiss_meta\n",
    "    \n",
    "    if vectorstore is None:\n",
    "        raise RuntimeError(\"Call load_or_create_faiss() before add_documents_to_faiss()\")\n",
    "    \n",
    "    new_docs: List[Document] = []\n",
    "    \n",
    "    for d in docs:\n",
    "        key = fingerprint(d.page_content, d.metadata or {})\n",
    "        if key in faiss_meta[\"rows\"]:\n",
    "            continue  # Skip duplicate\n",
    "        faiss_meta[\"rows\"][key] = True\n",
    "        new_docs.append(d)\n",
    "    \n",
    "    if new_docs:\n",
    "        vectorstore.add_documents(new_docs)\n",
    "        vectorstore.save_local(str(faiss_index_dir))\n",
    "        save_meta()\n",
    "        log.info(\"Documents added to FAISS\", added=len(new_docs), skipped=len(docs)-len(new_docs))\n",
    "    \n",
    "    return len(new_docs)\n",
    "\n",
    "print(\"add_documents_to_faiss function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba6260",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: Chat Ingestor Functions\n",
    "\n",
    "The Chat Ingestor handles:\n",
    "- Saving uploaded files\n",
    "- Loading documents (PDF, DOCX, TXT)\n",
    "- Splitting into chunks\n",
    "- Building a retriever for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0dee6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Chat Ingestor - Setup Directories\n",
    "\n",
    "**Purpose:** Set up directory structure for document ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a13775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Ingestor directory setup\n",
    "temp_base = Path(TEMP_BASE)\n",
    "temp_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "faiss_base = Path(FAISS_BASE)\n",
    "faiss_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Session-specific directories\n",
    "if USE_SESSION_DIRS:\n",
    "    temp_dir = temp_base / session_id\n",
    "    faiss_dir = faiss_base / session_id\n",
    "else:\n",
    "    temp_dir = temp_base\n",
    "    faiss_dir = faiss_base\n",
    "\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Temp directory: {temp_dir}\")\n",
    "print(f\"FAISS directory: {faiss_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09285771",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Chat Ingestor - Split Documents\n",
    "\n",
    "**Purpose:** Split documents into smaller chunks for better retrieval.\n",
    "\n",
    "**Why chunking matters:**\n",
    "- LLMs have context limits\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Overlap ensures context isn't lost at boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(docs: List[Document], \n",
    "                    chunk_size: int = CHUNK_SIZE, \n",
    "                    chunk_overlap: int = CHUNK_OVERLAP) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of LangChain Document objects\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: Chunked documents\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    log.info(\"Documents split\", \n",
    "             input_docs=len(docs), \n",
    "             output_chunks=len(chunks), \n",
    "             chunk_size=chunk_size, \n",
    "             overlap=chunk_overlap)\n",
    "    return chunks\n",
    "\n",
    "print(f\"split_documents function defined (chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a727a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Chat Ingestor - Build Retriever\n",
    "\n",
    "**Purpose:** Complete pipeline to ingest uploaded files and create a retriever.\n",
    "\n",
    "**Pipeline:**\n",
    "```\n",
    "Uploaded Files → Save to Disk → Load as Documents → Split into Chunks\n",
    "    → Create/Update FAISS Index → Return Retriever\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e26295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retriever(uploaded_files: Iterable,\n",
    "                    chunk_size: int = CHUNK_SIZE,\n",
    "                    chunk_overlap: int = CHUNK_OVERLAP,\n",
    "                    k: int = RETRIEVER_K):\n",
    "    \"\"\"\n",
    "    Build a retriever from uploaded files.\n",
    "    \n",
    "    Args:\n",
    "        uploaded_files: Iterable of file-like objects\n",
    "        chunk_size: Characters per chunk\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve per query\n",
    "        \n",
    "    Returns:\n",
    "        Retriever: FAISS-based retriever ready for RAG\n",
    "    \"\"\"\n",
    "    global vectorstore, faiss_meta\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Save uploaded files\n",
    "        paths = save_uploaded_files(uploaded_files, temp_dir)\n",
    "        print(f\"Step 1: Saved {len(paths)} files to {temp_dir}\")\n",
    "        \n",
    "        # Step 2: Load documents\n",
    "        docs = load_documents(paths)\n",
    "        if not docs:\n",
    "            raise ValueError(\"No valid documents loaded\")\n",
    "        print(f\"Step 2: Loaded {len(docs)} documents\")\n",
    "        \n",
    "        # Step 3: Split into chunks\n",
    "        chunks = split_documents(docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        print(f\"Step 3: Split into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Step 4: Prepare for FAISS\n",
    "        texts = [c.page_content for c in chunks]\n",
    "        metas = [c.metadata for c in chunks]\n",
    "        \n",
    "        # Step 5: Create/load FAISS index\n",
    "        # Reset index dir to match current session\n",
    "        global faiss_index_dir, meta_path\n",
    "        faiss_index_dir = faiss_dir\n",
    "        meta_path = faiss_index_dir / \"ingested_meta.json\"\n",
    "        \n",
    "        if meta_path.exists():\n",
    "            faiss_meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) or {\"rows\": {}}\n",
    "        else:\n",
    "            faiss_meta = {\"rows\": {}}\n",
    "        \n",
    "        vs = load_or_create_faiss(texts=texts, metadatas=metas)\n",
    "        print(f\"Step 4: FAISS index ready at {faiss_index_dir}\")\n",
    "        \n",
    "        # Step 6: Add documents (idempotent)\n",
    "        added = add_documents_to_faiss(chunks)\n",
    "        print(f\"Step 5: Added {added} new documents (skipped duplicates)\")\n",
    "        \n",
    "        # Step 7: Return retriever\n",
    "        retriever = vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "        log.info(\"Retriever built successfully\", k=k, index=str(faiss_dir))\n",
    "        print(f\"Step 6: Retriever ready (k={k})\")\n",
    "        \n",
    "        return retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR building retriever: {e}\")\n",
    "        log.error(\"Failed to build retriever\", error=str(e))\n",
    "        raise DocumentPortalException(\"Failed to build retriever\", e) from e\n",
    "\n",
    "print(\"build_retriever function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6c362",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: DocHandler Functions\n",
    "\n",
    "DocHandler is for **document analysis** - simpler than chat ingestion:\n",
    "- Save a single PDF\n",
    "- Read it page-by-page\n",
    "- Return text content for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39c3df",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13: DocHandler - Setup\n",
    "\n",
    "**Purpose:** Set up directory for document analysis storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocHandler setup\n",
    "doc_handler_data_dir = os.getenv(\"DATA_STORAGE_PATH\", ANALYSIS_DIR)\n",
    "doc_handler_session_id = session_id\n",
    "doc_handler_session_path = os.path.join(doc_handler_data_dir, doc_handler_session_id)\n",
    "os.makedirs(doc_handler_session_path, exist_ok=True)\n",
    "\n",
    "print(f\"DocHandler session path: {doc_handler_session_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56253f1c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 14: DocHandler - Save PDF\n",
    "\n",
    "**Purpose:** Save an uploaded PDF file to the session directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a119ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pdf_for_analysis(uploaded_file) -> str:\n",
    "    \"\"\"\n",
    "    Save uploaded PDF to session directory for analysis.\n",
    "    \n",
    "    Args:\n",
    "        uploaded_file: File-like object with .name attribute\n",
    "        \n",
    "    Returns:\n",
    "        str: Path where file was saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(uploaded_file.name)\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            raise ValueError(\"Invalid file type. Only PDFs are allowed.\")\n",
    "        \n",
    "        save_path = os.path.join(doc_handler_session_path, filename)\n",
    "        \n",
    "        with open(save_path, \"wb\") as f:\n",
    "            if hasattr(uploaded_file, \"read\"):\n",
    "                f.write(uploaded_file.read())\n",
    "            else:\n",
    "                f.write(uploaded_file.getbuffer())\n",
    "        \n",
    "        log.info(\"PDF saved for analysis\", file=filename, path=save_path)\n",
    "        return save_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving PDF: {e}\")\n",
    "        log.error(\"Failed to save PDF\", error=str(e))\n",
    "        raise DocumentPortalException(f\"Failed to save PDF: {str(e)}\", e) from e\n",
    "\n",
    "print(\"save_pdf_for_analysis function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0db226",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 15: DocHandler - Read PDF\n",
    "\n",
    "**Purpose:** Read PDF content page-by-page using PyMuPDF.\n",
    "\n",
    "**Output format:**\n",
    "```\n",
    "--- Page 1 ---\n",
    "[page 1 content]\n",
    "\n",
    "--- Page 2 ---\n",
    "[page 2 content]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_for_analysis(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read PDF content page-by-page.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Full text content with page markers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text_chunks = []\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page_num in range(doc.page_count):\n",
    "                page = doc.load_page(page_num)\n",
    "                text_chunks.append(f\"\\n--- Page {page_num + 1} ---\\n{page.get_text()}\")\n",
    "        \n",
    "        text = \"\\n\".join(text_chunks)\n",
    "        log.info(\"PDF read for analysis\", path=pdf_path, pages=len(text_chunks))\n",
    "        return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading PDF: {e}\")\n",
    "        log.error(\"Failed to read PDF\", error=str(e), path=pdf_path)\n",
    "        raise DocumentPortalException(f\"Could not process PDF: {pdf_path}\", e) from e\n",
    "\n",
    "print(\"read_pdf_for_analysis function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772e50a",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: Document Comparator Functions\n",
    "\n",
    "DocumentComparator handles **comparing two PDF documents**:\n",
    "- Save both PDFs to session directory\n",
    "- Read each PDF\n",
    "- Combine content for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c863c8d",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 16: DocumentComparator - Setup\n",
    "\n",
    "**Purpose:** Set up directory for document comparison storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentComparator setup\n",
    "compare_base_dir = Path(COMPARE_DIR)\n",
    "compare_session_id = session_id\n",
    "compare_session_path = compare_base_dir / compare_session_id\n",
    "compare_session_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"DocumentComparator session path: {compare_session_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af15dac",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 17: DocumentComparator - Save Files\n",
    "\n",
    "**Purpose:** Save two PDF files (reference and actual) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_comparison_files(reference_file, actual_file) -> tuple:\n",
    "    \"\"\"\n",
    "    Save two PDF files for comparison.\n",
    "    \n",
    "    Args:\n",
    "        reference_file: The original/reference document\n",
    "        actual_file: The document to compare against reference\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (reference_path, actual_path)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ref_path = compare_session_path / reference_file.name\n",
    "        act_path = compare_session_path / actual_file.name\n",
    "        \n",
    "        for fobj, out in ((reference_file, ref_path), (actual_file, act_path)):\n",
    "            if not fobj.name.lower().endswith(\".pdf\"):\n",
    "                raise ValueError(\"Only PDF files are allowed.\")\n",
    "            with open(out, \"wb\") as f:\n",
    "                if hasattr(fobj, \"read\"):\n",
    "                    f.write(fobj.read())\n",
    "                else:\n",
    "                    f.write(fobj.getbuffer())\n",
    "        \n",
    "        log.info(\"Comparison files saved\", reference=str(ref_path), actual=str(act_path))\n",
    "        return ref_path, act_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving comparison files: {e}\")\n",
    "        log.error(\"Error saving PDF files\", error=str(e))\n",
    "        raise DocumentPortalException(\"Error saving files\", e) from e\n",
    "\n",
    "print(\"save_comparison_files function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b914b2c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 18: DocumentComparator - Read PDF\n",
    "\n",
    "**Purpose:** Read PDF for comparison (similar to DocHandler but with encryption check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_for_comparison(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Read PDF content for comparison.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Text content with page markers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            if doc.is_encrypted:\n",
    "                raise ValueError(f\"PDF is encrypted: {pdf_path.name}\")\n",
    "            \n",
    "            parts = []\n",
    "            for page_num in range(doc.page_count):\n",
    "                page = doc.load_page(page_num)\n",
    "                text = page.get_text()\n",
    "                if text.strip():\n",
    "                    parts.append(f\"\\n --- Page {page_num + 1} --- \\n{text}\")\n",
    "        \n",
    "        log.info(\"PDF read for comparison\", file=str(pdf_path), pages=len(parts))\n",
    "        return \"\\n\".join(parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading PDF: {e}\")\n",
    "        log.error(\"Error reading PDF\", file=str(pdf_path), error=str(e))\n",
    "        raise DocumentPortalException(\"Error reading PDF\", e) from e\n",
    "\n",
    "print(\"read_pdf_for_comparison function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af684e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 19: DocumentComparator - Combine Documents\n",
    "\n",
    "**Purpose:** Combine all PDFs in session directory into a single string for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_documents_for_comparison() -> str:\n",
    "    \"\"\"\n",
    "    Combine all PDF files in the comparison session directory.\n",
    "    \n",
    "    Returns:\n",
    "        str: Combined text from all PDFs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc_parts = []\n",
    "        for file in sorted(compare_session_path.iterdir()):\n",
    "            if file.is_file() and file.suffix.lower() == \".pdf\":\n",
    "                content = read_pdf_for_comparison(file)\n",
    "                doc_parts.append(f\"Document: {file.name}\\n{content}\")\n",
    "        \n",
    "        combined_text = \"\\n\\n\".join(doc_parts)\n",
    "        log.info(\"Documents combined\", count=len(doc_parts))\n",
    "        return combined_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR combining documents: {e}\")\n",
    "        log.error(\"Error combining documents\", error=str(e))\n",
    "        raise DocumentPortalException(\"Error combining documents\", e) from e\n",
    "\n",
    "print(\"combine_documents_for_comparison function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e50caf",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 20: DocumentComparator - Clean Old Sessions\n",
    "\n",
    "**Purpose:** Delete old session directories to free up disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17914938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_old_sessions(keep_latest: int = 3):\n",
    "    \"\"\"\n",
    "    Delete old session directories, keeping only the latest N.\n",
    "    \n",
    "    Args:\n",
    "        keep_latest: Number of recent sessions to keep\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sessions = sorted([f for f in compare_base_dir.iterdir() if f.is_dir()], reverse=True)\n",
    "        deleted = 0\n",
    "        for folder in sessions[keep_latest:]:\n",
    "            shutil.rmtree(folder, ignore_errors=True)\n",
    "            log.info(\"Old session deleted\", path=str(folder))\n",
    "            deleted += 1\n",
    "        print(f\"Cleaned {deleted} old sessions (kept latest {keep_latest})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR cleaning sessions: {e}\")\n",
    "        log.error(\"Error cleaning old sessions\", error=str(e))\n",
    "        raise DocumentPortalException(\"Error cleaning old sessions\", e) from e\n",
    "\n",
    "print(\"clean_old_sessions function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea68dd",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: Testing Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04ce3a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 21: Test - Create Sample Document and Test FAISS\n",
    "\n",
    "**Purpose:** Test FAISS index creation with sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8764e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: FAISS Index Creation\n",
    "# ============================================================\n",
    "\n",
    "# Sample documents\n",
    "sample_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with many layers.\",\n",
    "    \"Natural language processing enables computers to understand text.\",\n",
    "    \"Computer vision allows machines to interpret images.\"\n",
    "]\n",
    "\n",
    "sample_metas = [\n",
    "    {\"source\": \"ml_intro.txt\", \"row_id\": 0},\n",
    "    {\"source\": \"dl_intro.txt\", \"row_id\": 0},\n",
    "    {\"source\": \"nlp_intro.txt\", \"row_id\": 0},\n",
    "    {\"source\": \"cv_intro.txt\", \"row_id\": 0},\n",
    "]\n",
    "\n",
    "print(\"Testing FAISS index creation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create/load FAISS index\n",
    "vs = load_or_create_faiss(texts=sample_texts, metadatas=sample_metas)\n",
    "\n",
    "# Create Document objects for adding\n",
    "sample_docs = [Document(page_content=t, metadata=m) for t, m in zip(sample_texts, sample_metas)]\n",
    "added = add_documents_to_faiss(sample_docs)\n",
    "print(f\"\\nAdded {added} new documents\")\n",
    "\n",
    "# Test search\n",
    "print(\"\\nTesting similarity search:\")\n",
    "query = \"What is machine learning?\"\n",
    "results = vs.similarity_search(query, k=2)\n",
    "print(f\"Query: '{query}'\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  Result {i}: {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5bcf3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 22: Test - Read Existing PDF (if available)\n",
    "\n",
    "**Purpose:** Test PDF reading with an existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60026a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: PDF Reading\n",
    "# ============================================================\n",
    "\n",
    "TEST_PDF_PATH = \"data/multi_doc_chat/state_of_the_union.txt\"  # <-- CHANGE THIS to a real PDF\n",
    "\n",
    "if os.path.exists(TEST_PDF_PATH) and TEST_PDF_PATH.lower().endswith('.pdf'):\n",
    "    print(f\"Reading: {TEST_PDF_PATH}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    content = read_pdf_for_analysis(TEST_PDF_PATH)\n",
    "    print(f\"Total characters: {len(content)}\")\n",
    "    print(f\"\\nFirst 500 characters:\")\n",
    "    print(content[:500])\n",
    "else:\n",
    "    print(f\"Test PDF not found: {TEST_PDF_PATH}\")\n",
    "    print(\"Update TEST_PDF_PATH to test PDF reading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50448ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Variables\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `embeddings` | Embeddings | Embedding model for vectorization |\n",
    "| `vectorstore` | FAISS | The vector index |\n",
    "| `faiss_meta` | dict | Tracks ingested documents |\n",
    "| `session_id` | str | Current session identifier |\n",
    "\n",
    "### Functions by Category\n",
    "\n",
    "**FAISS Manager:**\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `fingerprint(text, meta)` | Generate unique doc ID |\n",
    "| `load_or_create_faiss(texts, metas)` | Load/create FAISS index |\n",
    "| `add_documents_to_faiss(docs)` | Add docs idempotently |\n",
    "\n",
    "**Chat Ingestor:**\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `split_documents(docs)` | Chunk documents |\n",
    "| `build_retriever(files)` | Full ingestion pipeline |\n",
    "\n",
    "**DocHandler:**\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `save_pdf_for_analysis(file)` | Save uploaded PDF |\n",
    "| `read_pdf_for_analysis(path)` | Read PDF content |\n",
    "\n",
    "**DocumentComparator:**\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `save_comparison_files(ref, act)` | Save two PDFs |\n",
    "| `read_pdf_for_comparison(path)` | Read PDF for comparison |\n",
    "| `combine_documents_for_comparison()` | Combine all PDFs |\n",
    "| `clean_old_sessions(keep)` | Delete old sessions |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
