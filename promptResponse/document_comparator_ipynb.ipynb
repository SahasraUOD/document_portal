{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f52bc78",
   "metadata": {},
   "source": [
    "# Document Comparator - Step-by-Step Notebook\n",
    "\n",
    "This notebook converts `src/document_compare/document_comparator.py` into a procedural, debuggable format.\n",
    "\n",
    "## What This Does\n",
    "Compares two PDF documents using an LLM to identify **page-by-page differences**:\n",
    "- Analyzes content from both documents\n",
    "- Identifies changes on each page\n",
    "- Returns structured output as a DataFrame\n",
    "\n",
    "## Prerequisites\n",
    "- API keys set: `GOOGLE_API_KEY` and/or `GROQ_API_KEY`\n",
    "- Config file at `config/config.yaml`\n",
    "- Two documents (as combined text) to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48fc80",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Configuration Placeholders\n",
    "\n",
    "**Purpose:** Define sample combined document text for testing. Replace with your actual document content.\n",
    "\n",
    "**External Dependencies:**\n",
    "- Environment variables: `GOOGLE_API_KEY`, `GROQ_API_KEY`, `LLM_PROVIDER`\n",
    "\n",
    "**Input Format:** The `combined_docs` should contain content from both PDFs, typically formatted as:\n",
    "```\n",
    "=== DOCUMENT 1 ===\n",
    "Page 1: ...\n",
    "Page 2: ...\n",
    "\n",
    "=== DOCUMENT 2 ===\n",
    "Page 1: ...\n",
    "Page 2: ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION PLACEHOLDERS - MODIFY THESE BEFORE RUNNING\n",
    "# ============================================================\n",
    "\n",
    "# Sample combined documents for testing (replace with your actual content)\n",
    "SAMPLE_COMBINED_DOCS = \"\"\"\n",
    "=== DOCUMENT 1 (Original Version) ===\n",
    "\n",
    "Page 1:\n",
    "Company Policy Manual v1.0\n",
    "Effective Date: January 1, 2024\n",
    "Section 1: Employee Benefits\n",
    "All full-time employees are entitled to 15 days of paid vacation per year.\n",
    "Health insurance coverage begins after 30 days of employment.\n",
    "\n",
    "Page 2:\n",
    "Section 2: Work Hours\n",
    "Standard work hours are 9:00 AM to 5:00 PM, Monday through Friday.\n",
    "Overtime requires manager approval.\n",
    "\n",
    "=== DOCUMENT 2 (Updated Version) ===\n",
    "\n",
    "Page 1:\n",
    "Company Policy Manual v2.0\n",
    "Effective Date: January 1, 2025\n",
    "Section 1: Employee Benefits\n",
    "All full-time employees are entitled to 20 days of paid vacation per year.\n",
    "Health insurance coverage begins immediately upon employment.\n",
    "\n",
    "Page 2:\n",
    "Section 2: Work Hours\n",
    "Standard work hours are 9:00 AM to 5:00 PM, Monday through Friday.\n",
    "Remote work options available with manager approval.\n",
    "Overtime requires manager approval.\n",
    "\"\"\"  # <-- REPLACE WITH YOUR COMBINED DOCUMENT CONTENT\n",
    "\n",
    "print(f\"Combined documents preview ({len(SAMPLE_COMBINED_DOCS)} chars):\")\n",
    "print(SAMPLE_COMBINED_DOCS[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba4860",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Imports\n",
    "\n",
    "**Purpose:** Import all required libraries and modules.\n",
    "\n",
    "**Key Dependencies:**\n",
    "- `pandas`: For structuring comparison results as DataFrame\n",
    "- `langchain_core.output_parsers.JsonOutputParser`: Parses LLM output as JSON\n",
    "- `langchain.output_parsers.OutputFixingParser`: Auto-fixes malformed JSON\n",
    "- `model.models.SummaryResponse`: Pydantic model for comparison output schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fabfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from prompt.prompt_library import PROMPT_REGISTRY\n",
    "from model.models import SummaryResponse, PromptType\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc5703",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Understanding the Output Schema\n",
    "\n",
    "**Purpose:** Show the expected output structure from the document comparator.\n",
    "\n",
    "The `SummaryResponse` model expects a list of `ChangeFormat` objects:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `Page` | `str` | Page number where change was found |\n",
    "| `Changes` | `str` | Description of what changed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682be168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.models import ChangeFormat\n",
    "\n",
    "# Display the schema\n",
    "print(\"ChangeFormat Schema:\")\n",
    "print(\"=\"*60)\n",
    "for field_name, field_info in ChangeFormat.model_fields.items():\n",
    "    print(f\"  {field_name}: {field_info.annotation}\")\n",
    "\n",
    "print(\"\\nExpected LLM output format:\")\n",
    "print(\"=\"*60)\n",
    "example_output = [\n",
    "    {\"Page\": \"1\", \"Changes\": \"Version updated from v1.0 to v2.0; Vacation days increased from 15 to 20\"},\n",
    "    {\"Page\": \"2\", \"Changes\": \"Added remote work options paragraph\"}\n",
    "]\n",
    "print(example_output)\n",
    "\n",
    "print(\"\\nAs DataFrame:\")\n",
    "print(pd.DataFrame(example_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98184fe1",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Load LLM Model\n",
    "\n",
    "**Purpose:** Initialize the Language Model that will compare documents.\n",
    "\n",
    "**What happens:**\n",
    "- `ModelLoader` reads `config/config.yaml` for model settings\n",
    "- Checks `LLM_PROVIDER` env var (default: \"google\")\n",
    "- Returns either `ChatGoogleGenerativeAI` or `ChatGroq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa89b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM\n",
    "try:\n",
    "    loader = ModelLoader()\n",
    "    llm = loader.load_llm()\n",
    "    \n",
    "    if not llm:\n",
    "        raise ValueError(\"LLM could not be loaded\")\n",
    "    \n",
    "    log.info(\"LLM loaded successfully\", model=llm)\n",
    "    print(f\"LLM loaded: {type(llm).__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading LLM: {e}\")\n",
    "    raise DocumentPortalException(\"LLM loading error\", sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da2ab6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Initialize Output Parsers\n",
    "\n",
    "**Purpose:** Set up parsers to convert LLM text output into structured JSON.\n",
    "\n",
    "**Two parsers:**\n",
    "1. **JsonOutputParser**: Primary parser expecting JSON matching `SummaryResponse` schema\n",
    "2. **OutputFixingParser**: Backup parser that auto-fixes malformed JSON\n",
    "\n",
    "**Note:** The comparison chain uses the primary `parser` (not fixing_parser) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parsers\n",
    "parser = JsonOutputParser(pydantic_object=SummaryResponse)\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
    "\n",
    "print(\"Parsers initialized:\")\n",
    "print(f\"  - JsonOutputParser (expects SummaryResponse schema)\")\n",
    "print(f\"  - OutputFixingParser (available for fixing malformed JSON)\")\n",
    "\n",
    "# Show format instructions\n",
    "print(\"\\nFormat instructions for LLM:\")\n",
    "print(\"=\"*60)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions[:600] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3d397",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Load Document Comparison Prompt\n",
    "\n",
    "**Purpose:** Load the prompt template that tells the LLM how to compare documents.\n",
    "\n",
    "**The prompt instructs the LLM to:**\n",
    "1. Compare content from two PDFs\n",
    "2. Identify differences and note page numbers\n",
    "3. Provide page-wise comparison\n",
    "4. Mark unchanged pages as 'NO CHANGE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document comparison prompt\n",
    "prompt = PROMPT_REGISTRY[PromptType.DOCUMENT_COMPARISON.value]\n",
    "\n",
    "print(\"Document Comparison Prompt loaded!\")\n",
    "print(\"\\nPrompt template:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0713cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Build the Comparison Chain\n",
    "\n",
    "**Purpose:** Create the LCEL chain that connects prompt → LLM → parser.\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Combined Documents + Format Instructions\n",
    "    ↓\n",
    "Prompt Template (fills placeholders)\n",
    "    ↓\n",
    "LLM (generates JSON comparison)\n",
    "    ↓\n",
    "JsonOutputParser (parses to list of dicts)\n",
    "    ↓\n",
    "[{\"Page\": \"1\", \"Changes\": \"...\"}, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45eefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the comparison chain\n",
    "comparison_chain = prompt | llm | parser\n",
    "\n",
    "log.info(\"DocumentComparatorLLM chain initialized\", model=llm)\n",
    "print(\"Comparison chain built successfully!\")\n",
    "print(\"\\nChain structure:\")\n",
    "print(\"  prompt → llm → parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20a408",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Helper Function - Format Response to DataFrame\n",
    "\n",
    "**Purpose:** Convert the parsed LLM response (list of dicts) into a pandas DataFrame.\n",
    "\n",
    "**Input:** List of dictionaries with 'Page' and 'Changes' keys\n",
    "\n",
    "**Output:** pandas DataFrame for easy viewing and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4388f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response_parsed: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert parsed LLM response into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        response_parsed: List of dicts with 'Page' and 'Changes' keys\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Formatted comparison results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(response_parsed)\n",
    "        log.info(\"Response formatted to DataFrame\", rows=len(df))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR formatting response: {e}\")\n",
    "        log.error(\"Error formatting response into DataFrame\", error=str(e))\n",
    "        raise DocumentPortalException(\"Error formatting response\", sys)\n",
    "\n",
    "print(\"format_response function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354e412",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Main Function - Compare Documents\n",
    "\n",
    "**Purpose:** Wrapper function to invoke the comparison chain with proper error handling.\n",
    "\n",
    "**Parameters:**\n",
    "- `combined_docs`: String containing content from both documents to compare\n",
    "\n",
    "**Returns:** pandas DataFrame with page-by-page comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_documents(combined_docs: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare two documents and return page-wise differences.\n",
    "    \n",
    "    Args:\n",
    "        combined_docs: String containing both documents' content\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Comparison results with columns ['Page', 'Changes']\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = {\n",
    "            \"combined_docs\": combined_docs,\n",
    "            \"format_instruction\": parser.get_format_instructions()\n",
    "        }\n",
    "        \n",
    "        log.info(\"Invoking document comparison LLM chain\")\n",
    "        response = comparison_chain.invoke(inputs)\n",
    "        log.info(\"Chain invoked successfully\", response_preview=str(response)[:200])\n",
    "        \n",
    "        return format_response(response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR comparing documents: {e}\")\n",
    "        log.error(\"Error in compare_documents\", error=str(e))\n",
    "        raise DocumentPortalException(\"Error comparing documents\", sys)\n",
    "\n",
    "print(\"compare_documents function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef311f",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Test - Compare Sample Documents\n",
    "\n",
    "**Purpose:** Test the comparison chain with the sample documents from Cell 1.\n",
    "\n",
    "**Expected output:** DataFrame showing page-by-page differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f882fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: Compare Sample Documents\n",
    "# ============================================================\n",
    "\n",
    "print(\"Comparing documents...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result_df = compare_documents(SAMPLE_COMBINED_DOCS)\n",
    "\n",
    "print(\"\\nComparison Results:\")\n",
    "print(\"=\"*60)\n",
    "print(result_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caade62",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Test - Compare Documents from Files\n",
    "\n",
    "**Purpose:** Load and compare documents from actual PDF files.\n",
    "\n",
    "**Note:** This requires the document loading utilities from your project.\n",
    "Modify the file paths to point to your actual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: Compare Documents from Files\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# File paths (CHANGE THESE)\n",
    "DOC1_PATH = \"data/document_compare/document1.pdf\"  # <-- CHANGE THIS\n",
    "DOC2_PATH = \"data/document_compare/document2.pdf\"  # <-- CHANGE THIS\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(DOC1_PATH) or not os.path.exists(DOC2_PATH):\n",
    "    print(f\"Files not found:\")\n",
    "    print(f\"  Doc 1: {DOC1_PATH} - {'EXISTS' if os.path.exists(DOC1_PATH) else 'NOT FOUND'}\")\n",
    "    print(f\"  Doc 2: {DOC2_PATH} - {'EXISTS' if os.path.exists(DOC2_PATH) else 'NOT FOUND'}\")\n",
    "    print(\"\\nSkipping file-based test. Update paths to test with real files.\")\n",
    "else:\n",
    "    # You would load documents here using your document loading utilities\n",
    "    # Example (pseudo-code):\n",
    "    # from utils.document_ops import load_pdf_text\n",
    "    # doc1_text = load_pdf_text(DOC1_PATH)\n",
    "    # doc2_text = load_pdf_text(DOC2_PATH)\n",
    "    # combined = f\"=== DOCUMENT 1 ===\\n{doc1_text}\\n\\n=== DOCUMENT 2 ===\\n{doc2_text}\"\n",
    "    # result = compare_documents(combined)\n",
    "    print(\"Files found! Implement document loading to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a1d90",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Debug - Inspect Raw LLM Response\n",
    "\n",
    "**Purpose:** See the raw LLM output before parsing, useful for debugging JSON issues.\n",
    "\n",
    "This bypasses the parser to show exactly what the LLM returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8968d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEBUG: Inspect Raw LLM Response\n",
    "# ============================================================\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chain without parser (raw output)\n",
    "raw_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Getting raw LLM response (before parsing)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "raw_response = raw_chain.invoke({\n",
    "    \"combined_docs\": SAMPLE_COMBINED_DOCS,\n",
    "    \"format_instruction\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(\"\\nRaw LLM Response:\")\n",
    "print(\"-\"*60)\n",
    "print(raw_response)\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Try parsing manually\n",
    "print(\"\\nAttempting to parse...\")\n",
    "try:\n",
    "    import json\n",
    "    parsed = json.loads(raw_response)\n",
    "    print(\"✓ Valid JSON!\")\n",
    "    print(f\"Number of changes found: {len(parsed)}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"✗ Invalid JSON: {e}\")\n",
    "    print(\"\\nTrying with OutputFixingParser...\")\n",
    "    fixed = fixing_parser.parse(raw_response)\n",
    "    print(f\"Fixed result: {fixed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7db612",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13: Debug - Test with Different Document Pairs\n",
    "\n",
    "**Purpose:** Test the comparator with different types of document changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a946d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEBUG: Test Different Document Pairs\n",
    "# ============================================================\n",
    "\n",
    "test_pairs = {\n",
    "    \"Minor Changes\": \"\"\"\n",
    "=== DOCUMENT 1 ===\n",
    "Page 1: The quick brown fox jumps over the lazy dog.\n",
    "\n",
    "=== DOCUMENT 2 ===\n",
    "Page 1: The quick red fox jumps over the lazy dog.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Major Rewrite\": \"\"\"\n",
    "=== DOCUMENT 1 ===\n",
    "Page 1: Introduction to Machine Learning\n",
    "This chapter covers basic concepts.\n",
    "\n",
    "=== DOCUMENT 2 ===\n",
    "Page 1: Deep Learning Fundamentals\n",
    "This chapter has been completely rewritten to focus on neural networks.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Identical Documents\": \"\"\"\n",
    "=== DOCUMENT 1 ===\n",
    "Page 1: Same content in both documents.\n",
    "\n",
    "=== DOCUMENT 2 ===\n",
    "Page 1: Same content in both documents.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for test_name, combined_docs in test_pairs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {test_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        result = compare_documents(combined_docs)\n",
    "        print(result.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a9722",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 14: Export Results to CSV/Excel\n",
    "\n",
    "**Purpose:** Save the comparison results to a file for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20987ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPORT: Save Results to File\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_DIR = \"data/document_compare\"  # <-- CHANGE THIS\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"comparison_result.csv\")\n",
    "OUTPUT_EXCEL = os.path.join(OUTPUT_DIR, \"comparison_result.xlsx\")\n",
    "\n",
    "# Run comparison\n",
    "result_df = compare_documents(SAMPLE_COMBINED_DOCS)\n",
    "\n",
    "# Add metadata\n",
    "result_df['Comparison_Date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Results saved to CSV: {OUTPUT_CSV}\")\n",
    "\n",
    "# Save to Excel (if openpyxl is installed)\n",
    "try:\n",
    "    result_df.to_excel(OUTPUT_EXCEL, index=False)\n",
    "    print(f\"Results saved to Excel: {OUTPUT_EXCEL}\")\n",
    "except ImportError:\n",
    "    print(\"Note: Install openpyxl to export to Excel: pip install openpyxl\")\n",
    "\n",
    "print(\"\\nExported DataFrame:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589d0d5",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Variables Persisting Between Cells\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `llm` | LLM | Language model instance |\n",
    "| `parser` | JsonOutputParser | Parses LLM output to JSON |\n",
    "| `fixing_parser` | OutputFixingParser | Auto-fixes malformed JSON |\n",
    "| `prompt` | ChatPromptTemplate | Document comparison prompt template |\n",
    "| `comparison_chain` | Chain | Complete comparison pipeline |\n",
    "\n",
    "### Functions\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `format_response(response)` | Convert list of dicts to DataFrame |\n",
    "| `compare_documents(combined_docs)` | Main comparison function |\n",
    "\n",
    "### Output Schema\n",
    "\n",
    "```python\n",
    "# LLM returns list of:\n",
    "[{\"Page\": \"1\", \"Changes\": \"Description of changes\"},\n",
    " {\"Page\": \"2\", \"Changes\": \"NO CHANGE\"},\n",
    " ...]\n",
    "\n",
    "# Converted to DataFrame:\n",
    "#    Page                      Changes\n",
    "# 0     1    Description of changes\n",
    "# 1     2                  NO CHANGE\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
