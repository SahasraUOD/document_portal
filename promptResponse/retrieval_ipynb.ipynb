{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cf3d7e",
   "metadata": {},
   "source": [
    "# Conversational RAG - Step-by-Step Notebook\n",
    "\n",
    "This notebook converts `src/document_chat/retrieval.py` into a procedural, debuggable format.\n",
    "\n",
    "## What This Does\n",
    "Implements a **Conversational Retrieval-Augmented Generation (RAG)** pipeline that:\n",
    "1. Rewrites user questions using conversation context\n",
    "2. Retrieves relevant document chunks from a FAISS index\n",
    "3. Generates answers using an LLM\n",
    "\n",
    "## Prerequisites\n",
    "- FAISS index already created (via document ingestion)\n",
    "- API keys set: `GOOGLE_API_KEY` and/or `GROQ_API_KEY`\n",
    "- Config file at `config/config.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2daccf6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Configuration Placeholders\n",
    "\n",
    "**Purpose:** Define all configurable paths and parameters at the top for easy modification.\n",
    "\n",
    "**External Dependencies:**\n",
    "- `FAISS_INDEX_PATH`: Directory containing your FAISS index files (`index.faiss`, `index.pkl`)\n",
    "- Environment variables: `GOOGLE_API_KEY`, `GROQ_API_KEY`, `LLM_PROVIDER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION PLACEHOLDERS - MODIFY THESE BEFORE RUNNING\n",
    "# ============================================================\n",
    "\n",
    "# Path to your FAISS index directory\n",
    "FAISS_INDEX_PATH = \"data/single_document_chat/your_session_id\"  # <-- CHANGE THIS\n",
    "\n",
    "# Name of the FAISS index files (without extension)\n",
    "FAISS_INDEX_NAME = \"index\"\n",
    "\n",
    "# Number of documents to retrieve per query\n",
    "RETRIEVAL_K = 5\n",
    "\n",
    "# Search type: \"similarity\" (cosine) or \"mmr\" (diversity-focused)\n",
    "SEARCH_TYPE = \"similarity\"\n",
    "\n",
    "# Session identifier for logging\n",
    "SESSION_ID = \"notebook_debug_session\"\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  FAISS Index: {FAISS_INDEX_PATH}\")\n",
    "print(f\"  Retrieval K: {RETRIEVAL_K}\")\n",
    "print(f\"  Search Type: {SEARCH_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379e8e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Imports\n",
    "\n",
    "**Purpose:** Import all required libraries and modules.\n",
    "\n",
    "**Key Dependencies:**\n",
    "- `langchain_core`: Prompt templates, output parsers, message types\n",
    "- `langchain_community.vectorstores.FAISS`: Vector similarity search\n",
    "- `utils.model_loader.ModelLoader`: Loads LLM and embedding models\n",
    "- `prompt.prompt_library.PROMPT_REGISTRY`: Central prompt storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Project imports\n",
    "from utils.model_loader import ModelLoader\n",
    "from exception.custom_exception import DocumentPortalException\n",
    "from logger import GLOBAL_LOGGER as log\n",
    "from prompt.prompt_library import PROMPT_REGISTRY\n",
    "from model.models import PromptType\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df0bcaa",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Load LLM Model\n",
    "\n",
    "**Purpose:** Initialize the Language Model that will:\n",
    "1. Rewrite questions with conversation context\n",
    "2. Generate answers from retrieved documents\n",
    "\n",
    "**What happens:**\n",
    "- `ModelLoader` reads `config/config.yaml` for model settings\n",
    "- Checks `LLM_PROVIDER` env var (default: \"google\")\n",
    "- Returns either `ChatGoogleGenerativeAI` or `ChatGroq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM\n",
    "try:\n",
    "    llm = ModelLoader().load_llm()\n",
    "    if not llm:\n",
    "        raise ValueError(\"LLM could not be loaded\")\n",
    "    log.info(\"LLM loaded successfully\", session_id=SESSION_ID)\n",
    "    print(f\"LLM loaded: {type(llm).__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading LLM: {e}\")\n",
    "    raise DocumentPortalException(\"LLM loading error\", sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fd1bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Load Prompt Templates\n",
    "\n",
    "**Purpose:** Load the two prompt templates from the registry:\n",
    "\n",
    "1. **contextualize_prompt**: Rewrites user questions to be standalone\n",
    "   - Input: User question + chat history\n",
    "   - Output: Standalone question (no context dependencies)\n",
    "\n",
    "2. **qa_prompt**: Generates answers from context\n",
    "   - Input: Retrieved documents + question + chat history\n",
    "   - Output: Concise answer (max 3 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompt templates from registry\n",
    "contextualize_prompt: ChatPromptTemplate = PROMPT_REGISTRY[\n",
    "    PromptType.CONTEXTUALIZE_QUESTION.value\n",
    "]\n",
    "qa_prompt: ChatPromptTemplate = PROMPT_REGISTRY[\n",
    "    PromptType.CONTEXT_QA.value\n",
    "]\n",
    "\n",
    "print(\"Prompts loaded successfully!\")\n",
    "print(f\"\\n--- Contextualize Prompt ---\")\n",
    "print(contextualize_prompt.messages[0].prompt.template[:200] + \"...\")\n",
    "print(f\"\\n--- QA Prompt ---\")\n",
    "print(qa_prompt.messages[0].prompt.template[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e82ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Load FAISS Retriever\n",
    "\n",
    "**Purpose:** Load the pre-built FAISS vector index from disk and create a retriever.\n",
    "\n",
    "**What happens:**\n",
    "1. Validates that `FAISS_INDEX_PATH` exists\n",
    "2. Loads embedding model (must match what was used during indexing)\n",
    "3. Loads FAISS index files (`index.faiss`, `index.pkl`)\n",
    "4. Creates retriever configured to return top-K similar documents\n",
    "\n",
    "**Note:** `allow_dangerous_deserialization=True` is needed because FAISS indexes contain pickled objects. Only use with trusted indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f310bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS retriever\n",
    "try:\n",
    "    # Validate path exists\n",
    "    if not os.path.isdir(FAISS_INDEX_PATH):\n",
    "        raise FileNotFoundError(f\"FAISS index directory not found: {FAISS_INDEX_PATH}\")\n",
    "    \n",
    "    # Load embeddings (must match what was used during indexing)\n",
    "    embeddings = ModelLoader().load_embeddings()\n",
    "    print(f\"Embeddings loaded: {type(embeddings).__name__}\")\n",
    "    \n",
    "    # Load FAISS vectorstore from disk\n",
    "    vectorstore = FAISS.load_local(\n",
    "        FAISS_INDEX_PATH,\n",
    "        embeddings,\n",
    "        index_name=FAISS_INDEX_NAME,\n",
    "        allow_dangerous_deserialization=True,  # Trust our own indexes\n",
    "    )\n",
    "    print(f\"FAISS vectorstore loaded from: {FAISS_INDEX_PATH}\")\n",
    "    \n",
    "    # Create retriever\n",
    "    search_kwargs = {\"k\": RETRIEVAL_K}\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=SEARCH_TYPE, \n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "    \n",
    "    log.info(\n",
    "        \"FAISS retriever loaded successfully\",\n",
    "        index_path=FAISS_INDEX_PATH,\n",
    "        index_name=FAISS_INDEX_NAME,\n",
    "        k=RETRIEVAL_K,\n",
    "        session_id=SESSION_ID,\n",
    "    )\n",
    "    print(f\"Retriever ready! Will return top {RETRIEVAL_K} documents per query.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"\\nMake sure to:\")\n",
    "    print(\"1. Update FAISS_INDEX_PATH in Cell 1\")\n",
    "    print(\"2. Run document ingestion first to create the index\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading retriever: {e}\")\n",
    "    raise DocumentPortalException(\"Loading error in retriever\", sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ce283",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Helper Function - Format Documents\n",
    "\n",
    "**Purpose:** Convert retrieved document objects into a single text string for the LLM context.\n",
    "\n",
    "**Input:** List of Document objects with `page_content` attribute\n",
    "\n",
    "**Output:** All documents joined with double newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs) -> str:\n",
    "    \"\"\"\n",
    "    Format retrieved documents into a single context string.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of Document objects from retriever\n",
    "        \n",
    "    Returns:\n",
    "        str: All document contents joined with '\\n\\n'\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(getattr(d, \"page_content\", str(d)) for d in docs)\n",
    "\n",
    "print(\"format_docs function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeca4bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Build LCEL Chain - Question Rewriter\n",
    "\n",
    "**Purpose:** Create the first stage of the pipeline that rewrites user questions.\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "{\"input\": question, \"chat_history\": [...]} \n",
    "    → contextualize_prompt (template)\n",
    "    → llm (rewrites question)\n",
    "    → StrOutputParser (extracts text)\n",
    "    → \"Standalone question\"\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Input: \"What happens if they're late?\" + history about payments\n",
    "- Output: \"What happens if payments are late?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Question Rewriter Chain\n",
    "question_rewriter = (\n",
    "    {\"input\": itemgetter(\"input\"), \"chat_history\": itemgetter(\"chat_history\")}\n",
    "    | contextualize_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Question rewriter chain built.\")\n",
    "print(\"\\nThis chain takes:\")\n",
    "print(\"  - 'input': The user's question\")\n",
    "print(\"  - 'chat_history': List of previous messages\")\n",
    "print(\"And returns: A standalone question string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006c86e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Build LCEL Chain - Document Retriever\n",
    "\n",
    "**Purpose:** Chain the question rewriter to the retriever and document formatter.\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "question_rewriter output (standalone question)\n",
    "    → retriever (FAISS similarity search)\n",
    "    → format_docs (join into single string)\n",
    "    → \"Doc1 content\\n\\nDoc2 content\\n\\n...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Document Retrieval Chain\n",
    "retrieve_docs = question_rewriter | retriever | format_docs\n",
    "\n",
    "print(\"Document retrieval chain built.\")\n",
    "print(\"\\nThis chain:\")\n",
    "print(\"  1. Takes rewritten question from Stage 1\")\n",
    "print(\"  2. Searches FAISS for similar documents\")\n",
    "print(f\"  3. Returns top {RETRIEVAL_K} documents as a single string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d500402",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Build LCEL Chain - Full RAG Pipeline\n",
    "\n",
    "**Purpose:** Create the complete end-to-end RAG chain.\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "{\"input\": question, \"chat_history\": [...]}\n",
    "    ↓\n",
    "{\n",
    "  \"context\": retrieve_docs output,\n",
    "  \"input\": original question,\n",
    "  \"chat_history\": original history\n",
    "}\n",
    "    → qa_prompt (answer template)\n",
    "    → llm (generates answer)\n",
    "    → StrOutputParser (extracts text)\n",
    "    → \"The answer is...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cde553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Full RAG Chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieve_docs,\n",
    "        \"input\": itemgetter(\"input\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "log.info(\"LCEL graph built successfully\", session_id=SESSION_ID)\n",
    "print(\"Full RAG chain built successfully!\")\n",
    "print(\"\\nThe complete pipeline:\")\n",
    "print(\"  User Question → Rewrite → Retrieve → Generate Answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e614d",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Invoke Function\n",
    "\n",
    "**Purpose:** Create a wrapper function to invoke the RAG chain with proper error handling.\n",
    "\n",
    "**Parameters:**\n",
    "- `user_input`: The user's question\n",
    "- `chat_history`: List of previous messages (optional)\n",
    "\n",
    "**Returns:** The generated answer string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09446a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_rag(user_input: str, chat_history: Optional[List[BaseMessage]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Invoke the RAG pipeline to answer a question.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user's question\n",
    "        chat_history: Previous conversation messages (optional)\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated answer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chat_history = chat_history or []\n",
    "        payload = {\"input\": user_input, \"chat_history\": chat_history}\n",
    "        \n",
    "        answer = rag_chain.invoke(payload)\n",
    "        \n",
    "        if not answer:\n",
    "            log.warning(\n",
    "                \"No answer generated\", \n",
    "                user_input=user_input, \n",
    "                session_id=SESSION_ID\n",
    "            )\n",
    "            return \"No answer generated.\"\n",
    "        \n",
    "        log.info(\n",
    "            \"Chain invoked successfully\",\n",
    "            session_id=SESSION_ID,\n",
    "            user_input=user_input,\n",
    "            answer_preview=str(answer)[:150],\n",
    "        )\n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during invocation: {e}\")\n",
    "        log.error(\"Failed to invoke RAG\", error=str(e))\n",
    "        raise DocumentPortalException(\"Invocation error in RAG\", sys)\n",
    "\n",
    "print(\"invoke_rag function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3617f1c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Test - Single Question\n",
    "\n",
    "**Purpose:** Test the RAG pipeline with a single question (no history).\n",
    "\n",
    "**Modify the question below to match your indexed documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: Single Question (No History)\n",
    "# ============================================================\n",
    "\n",
    "test_question = \"What is the main topic of this document?\"  # <-- CHANGE THIS\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answer = invoke_rag(test_question, chat_history=[])\n",
    "\n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484294a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Test - Multi-Turn Conversation\n",
    "\n",
    "**Purpose:** Test the conversational aspect - asking follow-up questions that reference previous exchanges.\n",
    "\n",
    "**This demonstrates:**\n",
    "- Question rewriting with context\n",
    "- Chat history management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST: Multi-Turn Conversation\n",
    "# ============================================================\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# --- Turn 1 ---\n",
    "q1 = \"What are the key points discussed?\"  # <-- CHANGE THIS\n",
    "print(f\"Q1: {q1}\")\n",
    "\n",
    "a1 = invoke_rag(q1, chat_history=conversation_history)\n",
    "print(f\"A1: {a1}\")\n",
    "\n",
    "# Add to history\n",
    "conversation_history.extend([\n",
    "    HumanMessage(content=q1),\n",
    "    AIMessage(content=a1)\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# --- Turn 2 (Follow-up) ---\n",
    "q2 = \"Can you tell me more about that?\"  # <-- CHANGE THIS (refers to previous answer)\n",
    "print(f\"Q2: {q2}\")\n",
    "\n",
    "a2 = invoke_rag(q2, chat_history=conversation_history)\n",
    "print(f\"A2: {a2}\")\n",
    "\n",
    "# Add to history\n",
    "conversation_history.extend([\n",
    "    HumanMessage(content=q2),\n",
    "    AIMessage(content=a2)\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# --- Turn 3 (Another follow-up) ---\n",
    "q3 = \"Are there any exceptions?\"  # <-- CHANGE THIS\n",
    "print(f\"Q3: {q3}\")\n",
    "\n",
    "a3 = invoke_rag(q3, chat_history=conversation_history)\n",
    "print(f\"A3: {a3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4e926",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13: Debug - Inspect Intermediate Steps\n",
    "\n",
    "**Purpose:** Step-by-step debugging to see what happens at each stage.\n",
    "\n",
    "Useful for:\n",
    "- Verifying question rewriting works correctly\n",
    "- Checking what documents are being retrieved\n",
    "- Understanding why an answer might be wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEBUG: Inspect Each Stage\n",
    "# ============================================================\n",
    "\n",
    "debug_question = \"What happens if they're late?\"  # <-- CHANGE THIS\n",
    "debug_history = [\n",
    "    HumanMessage(content=\"What are the payment terms?\"),\n",
    "    AIMessage(content=\"Payment is due within 30 days.\")\n",
    "]\n",
    "\n",
    "print(\"Original Question:\", debug_question)\n",
    "print(\"Chat History:\", [(m.type, m.content[:50]) for m in debug_history])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Step 1: See how question gets rewritten\n",
    "print(\"\\n[STAGE 1] Question Rewriting\")\n",
    "rewritten = question_rewriter.invoke({\n",
    "    \"input\": debug_question,\n",
    "    \"chat_history\": debug_history\n",
    "})\n",
    "print(f\"Rewritten Question: {rewritten}\")\n",
    "\n",
    "# Step 2: See what documents are retrieved\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n[STAGE 2] Document Retrieval\")\n",
    "retrieved_docs = retriever.invoke(rewritten)\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    preview = doc.page_content[:150].replace('\\n', ' ')\n",
    "    print(f\"\\n  Doc {i}: {preview}...\")\n",
    "\n",
    "# Step 3: See the formatted context\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n[STAGE 3] Formatted Context (first 500 chars)\")\n",
    "context = format_docs(retrieved_docs)\n",
    "print(context[:500] + \"...\")\n",
    "\n",
    "# Step 4: Final answer\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n[STAGE 4] Final Answer\")\n",
    "final_answer = invoke_rag(debug_question, debug_history)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e01645",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 14: Interactive Chat Loop\n",
    "\n",
    "**Purpose:** Interactive testing - keep asking questions until you type 'quit'.\n",
    "\n",
    "**Note:** Run this cell and interact in the output area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INTERACTIVE: Chat Loop\n",
    "# ============================================================\n",
    "\n",
    "print(\"Interactive RAG Chat\")\n",
    "print(\"Type 'quit' to exit, 'clear' to reset history\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "interactive_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    if user_input.lower() == 'clear':\n",
    "        interactive_history = []\n",
    "        print(\"History cleared.\\n\")\n",
    "        continue\n",
    "    \n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        response = invoke_rag(user_input, interactive_history)\n",
    "        print(f\"\\nAssistant: {response}\\n\")\n",
    "        \n",
    "        # Update history\n",
    "        interactive_history.extend([\n",
    "            HumanMessage(content=user_input),\n",
    "            AIMessage(content=response)\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c6fc3",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Variables Persisting Between Cells\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `llm` | LLM | Language model instance |\n",
    "| `embeddings` | Embeddings | Embedding model for FAISS |\n",
    "| `retriever` | Retriever | FAISS-based document retriever |\n",
    "| `contextualize_prompt` | ChatPromptTemplate | Question rewriting prompt |\n",
    "| `qa_prompt` | ChatPromptTemplate | Answer generation prompt |\n",
    "| `question_rewriter` | Chain | Stage 1 chain |\n",
    "| `retrieve_docs` | Chain | Stage 2 chain |\n",
    "| `rag_chain` | Chain | Complete RAG pipeline |\n",
    "\n",
    "### Functions\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `format_docs(docs)` | Convert document list to string |\n",
    "| `invoke_rag(question, history)` | Execute the RAG pipeline |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
